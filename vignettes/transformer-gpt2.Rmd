---
title: "Using a GPT2 transformer model to get word predictability."
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{transformer-gpt2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Transformer models are a type of neural network architecture used for natural language processing tasks such as language translation and text generation. They were introduced in the 2017 paper "Attention Is All You Need" by researchers at Google. 

Large Language Models (LLMs) are pre-trained transformer models. These models have been trained on massive amounts of text data and can be fine-tuned to perform a variety of NLP tasks such as text classification, named entity recognition, question answering, etc.

There are two types of [language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling), causal and masked. Causal language modeling predicts the next word (or in fact [token]()) based on the preceeding words. GPT-2 (Generative Pre-trained Transformer 2) developed by [OpenAI](https://openai.com/) is an example of a causal language model.

One interesting side-effect of the these models is that the probability of a word given a certain context can be extracted. Here we take advantage that GPT-2 can give us the probability of a word given its left context which can be used to approximate the psycholinguistic measure of *surprisal* (citation).



```{r setup, message = FALSE}
library(pangoling)
library(tidytable)
library(tictoc)
```

First, let's see what words GPT-2 predicts given a specific context. [Hugging Face](https://huggingface.co/) provide access to pre-trained models, including freely available versions of different sizes of  [GPT-2](https://huggingface.co/gpt2). The function `get_causal_next_tokens_tbl()` will be default use the smallest version of [GPT-2](https://huggingface.co/gpt2), but this can be modified with the argument `model`.


```{r}
tic()
(df_pred <- get_causal_next_tokens_tbl("The apple doesn't fall far from the"))
toc()
```

The first time will take some time, and it will download some files. Afterwards, it will be much faster.

```{r}
tic()
(df_pred <- get_causal_next_tokens_tbl("The apple doesn't fall far from the"))
toc()
```

Notice that the tokens--that is the way GPT2 interprets words-- that are predicted start with `Ä `, this indicates that they are not the first word of a sentence.

In fact this is the way GPT2 interprets the sentence:

```{r}
get_tokens("The apple doesn't fall far from the")
```

Because they are log probabilities if we exponentiate them and we sum them we should get 1:

```{r}
sum(exp(df_pred$log_prob))
```

Because of approximation errors, this is not exactly one.


When doing tests, `sshleifer/tiny-gpt2` is quite useful since it's tiny. But notice that the predictions are quite bad.

```{r}
preload_causal("sshleifer/tiny-gpt2")
tic()
get_causal_next_tokens_tbl("The apple doesn't fall far from the", model = "sshleifer/tiny-gpt2")
toc()
```

The intended use of this package is the following. Given a (toy) dataset like this.

```{r, cache = TRUE}
sentences <- c("The apple doesn't fall far from the tree.", "Don't judge a book by its cover.")
df_sent <- strsplit(x = sentences, split = " ") |> 
  map_dfr(.f =  ~ data.frame(word = .x), .id = "sent_n")
df_sent
```

It's straight-forward to get the log-probability (`-suprisal`) of each word based on GPT-2.

```{r}
df_sent <- df_sent |>
  mutate(lp = get_causal_log_prob(word, by = sent_n))
df_sent
```





