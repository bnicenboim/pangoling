---
title: "Using a GPT2 transformer model to get word predictability"
bibliography: '`r system.file("REFERENCES.bib", package="pangoling")`'
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<!-- https://ropensci.org/blog/2019/12/08/precompute-vignettes/ -->

Transformer models are a type of neural network architecture used for natural
language processing tasks such as language translation and text generation. They
were introduced in the @vaswani2017attention paper "Attention Is All You Need". 

Large Language Models (LLMs) are a specific type of pre-trained transformer 
models. These models have been trained on massive amounts of text data and can 
be fine-tuned to perform a variety of NLP tasks such as text classification, 
named entity recognition, question answering, etc.

A causal language model (also called GPT-like, auto-regressive, or decoder 
model) is a type of large language model usually used for text-generation that 
can predict the next word (or more accurately, the next token) based on a 
preceding context. GPT-2 (Generative Pre-trained Transformer 2) developed by [OpenAI](https://openai.com/) is an example of a causal language model 
[see also @radford2019language].

One interesting side-effect of causal language models is that the (log) 
probability of a word given a certain context can be extracted from the models. 

Load the following packages first:

```{r setup, message = FALSE}
library(pangoling)
library(tidytable) # fast alternative to dplyr
library(tictoc) # measure time
```

Then let's examine which continuation GPT-2 predicts following a specific 
context. [Hugging Face](https://huggingface.co/) provide access to pre-trained
models, including freely available versions of different sizes of 
[GPT-2](https://huggingface.co/gpt2). The function 
`causal_next_tokens_pred_tbl()` will, by default, use the smallest version of
[GPT-2](https://huggingface.co/gpt2), but this can be modified with the argument
`model`.

Let's see what GPT-2 predicts following "The apple doesn't fall far from the".

```{r}
tic()
(df_pred <- causal_next_tokens_pred_tbl("The apple doesn't fall far from the"))
toc()
```

(The pretrained models and tokenizers will be downloaded from 
https://huggingface.co/ the first time they are used.)

The most likely continuation is "tree", which makes sense.
The first time a model is run, it will download some files that will be 
available for subsequent runs. However, every time we start a new R session and 
we run a model, it will take some time to store it in memory. Next runs in the 
same session are much faster. We can also preload a model with 
`causal_preload()`.

```{r}
tic()
(df_pred <- causal_next_tokens_pred_tbl("The apple doesn't fall far from the"))
toc()
```

Notice that the tokens--that is, the way GPT-2 interprets words-- that are 
predicted start with `Ġ`, this indicates that they are not the first word of a
sentence.

In fact this is the way GPT-2 interprets our context:

```{r}
tokenize_lst("The apple doesn't fall far from the")
```

Also notice that GPT-2 tokenizer interprets differently initial tokens from 
tokens that follow a space. A space in  a token is indicated with "Ġ".

```{r}
tokenize_lst("This is different from This")
```

It's also possible to decode the tokens to get "pure" text:

```{r}
tokenize_lst("This is different from This", decode = TRUE)
```

Going back to the initial example, because `causal_next_tokens_pred_tbl()` 
returns by default log natural probabilities, if we exponentiate them and we 
sum them, we should get 1:

```{r}
sum(exp(df_pred$pred))
```

Because of approximation errors, this is not exactly one.


When doing tests, 
[`sshleifer/tiny-gpt2`](https://huggingface.co/sshleifer/tiny-gpt2) is quite 
useful since it's much faster because it's a tiny model. But notice that the
predictions are quite bad.

```{r}
causal_preload("sshleifer/tiny-gpt2")
tic()
causal_next_tokens_pred_tbl("The apple doesn't fall far from the",
  model = "sshleifer/tiny-gpt2"
)
toc()
```


All in all, the package `pangoling` would be most useful in the following 
situation. (And see also the [worked-out example vignette](example.html).)

Given a (toy) dataset where sentences are organized with one word or short 
phrase in each row:
```{r, cache = TRUE}
sentences <- c(
  "The apple doesn't fall far from the tree.",
  "Don't judge a book by its cover."
)
df_sent <- strsplit(x = sentences, split = " ") |>
  map_dfr(.f = ~ data.frame(word = .x), .id = "sent_n")
df_sent
```

One can get the natural log-transformed probability of each word based on 
GPT-2 as follows:

```{r}
df_sent <- df_sent |>
  mutate(lp = causal_words_pred(word, by = sent_n))
df_sent
```

Notice that the `by` is inside the `causal_words_pred()` function. It's also 
possible to use `by` in the mutate call, or `group_by()`, but it will be slower.


The attentive reader might have noticed that the log-probability of "tree" here 
is not the same as the one presented before. This is because the actual word is 
`" tree."` (notice the space), which contains two tokens:

```{r}
tokenize_lst(" tree.")
```

The log-probability of `" tree."` is the sum of the log-probability of `" tree"`
given its context and `"."` given its context.

We can verify this in the following way.

```{r}
df_token_lp <- causal_tokens_pred_lst(
  "The apple doesn't fall far from the tree.") |>
  # convert the list into a data frame
  map_dfr(~ data.frame(token = names(.x), pred = .x))
df_token_lp

(tree_lp <- df_token_lp |> 
  # requires a Ġ because there is a space before
  filter(token == "Ġtree") |>
  pull())

(dot_lp <- df_token_lp |>
  # doesn't require a Ġ because there is no space before
  filter(token == ".") |>
  pull())

tree._lp <- df_sent |>
  filter(word == "tree.") |>
  pull()

# Test whether it is equal
all.equal(
  tree_lp + dot_lp,
  tree._lp
)
```

In a scenario as the one above, when one has a word-by-word text, and one wants 
to know the log-probability of each word, one doesn't have to worry about the
encoding or tokens, since the function `causal_words_pred()` takes care of it.

# References
