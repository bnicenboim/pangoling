---
title: "Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading"
bibliography: '`r system.file("REFERENCES.bib", package="pangoling")`'
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE
)
```

This vignette demonstrates how to use the `pangoling` package to estimate the effect of *surprisal* on reading times (RTs) in a psycholinguistic experiment. 

Surprisal is a measure of processing difficulty based on the unexpectedness of a word in its context. As introduced by @levy2008, surprisal is defined as $-\log_2 P(\text{word}|\text{context})$, where $P(\text{word}|\text{context})$ is the probability of the word given its preceding context. Higher surprisal values indicate greater processing difficulty and suggest a cognitive processing bottleneck, as readers allocate more resources to processing unexpected words. @levy2008 argues that human sentence comprehension operates efficiently by leveraging probabilistic expectations. When these expectations are violated (i.e., when surprisal is high), cognitive processing slows down, which can be observed through longer reading times in reading tasks.


Surprisal is computed here using a GPT-2-like causal language model trained on Chinese texts. We use `pangoling` to retrieve the surprisal values. The dataset comes from @JaegerEtAl2015, which features Chinese sentences presented word-by-word to participants during a self-paced reading experiment.


# Data analysis

## 0. Preprocessing

We load the required libraries first.

```{r, message = FALSE}
library(pangoling)
library(tidytable) # Lightweight and faster alternative to dplyr
library(tictoc)  # Report the time that a piece of code takes
library(brms) # Bayesian regression models in Stan
options(mc.cores = parallel::detectCores()) # Use all the cores needed available for the Bayesian analysis by default
```

The dataset `df_jaeger14` from @JaegerEtAl2015 contains Chinese sentences presented word-by-word to participants. Each word has an associated reaction time (RT). (The original research question and the experimental conditions don't matter to us.)

```{r}
data("df_jaeger14")
df_jaeger14
```

To avoid redundant computations, we extract unique combinations of items, conditions, and words. (We don't want to run `pangoling` on the entire dataset because there will be repetition across subjects, and thus the model will run several times on identical words in identical sentences):

```{r}
df_stim <- df_jaeger14 |> distinct(item, cond, word, wordn)
df_stim |> print(n = 20)
```



We use the `pangoling` function `causal_preload()` to load the GPT-2 model [`uer/gpt2-chinese-cluecorpussmall`](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall) This model is trained on Chinese texts and can predict word probabilities based on preceding context.

```{r}
causal_preload("uer/gpt2-chinese-cluecorpussmall")
```

## 1. Add surprisal values to the dataset

Surprisal in bits (i.e., we use a logarithm with base 2) is calculated as $-\log_2(P(\text{word}|\text{context}))$, which is equivalent to $\log_{1/2}(P(\text{word}|\text{context}))$. The `pangoling` function `causal_words_pred()` is configured to:

- Use `log.p = 0.5` to get surprisal in bits.
- Set `sep = ""` since Chinese words are not separated by spaces.
- Process sentences in batches of 10 for efficiency.

```{r calc-surp}
tic()
df_stim <- df_stim |>
  mutate(surprisal = causal_words_pred(
    word,
    by = interaction(item, cond),
    log.p = 0.5,
    model = "uer/gpt2-chinese-cluecorpussmall",
    sep = "",
    batch_size = 10
  ))
toc()
```

Inspect the calculated surprisal values:

```{r}
df_stim |> print(n = 20)
```

The surprisal values are merged back into the original dataset:

```{r}
df_jaeger14 <- df_jaeger14 |> left_join(df_stim)
```

## 2. Analyze the dataset with a Bayesian hierarchical model

We estimate the effect of surprisal on reading times (RTs) using a Bayesian hierarchical model. The model incorporates a hierarchical structure with by-subject and by-item group-level effects (also referred to as random effects), which account for variability across subjects and items. The only population-level effect (also called a fixed effect) of interest in this analysis is surprisal.
(For simplicity, we do not scale or center the predictor.)

For a detailed discussion of Bayesian hierarchical structures (including prior selection), see Section 4.2 and Chapter 5 of @nicenboim2025introduction, [*An Introduction to Bayesian Data Analysis for Cognitive Science*](https://bruno.nicenboim.me/bayescogsci/).



```{r brms, message = FALSE}
tic()
fit_surp <-
  brm(rt ~ surprisal + (surprisal| subject) + (surprisal| item),
      data = df_jaeger14,
      family = lognormal(),
      prior = c(prior(normal(6, 1.5), class = Intercept),
                prior(normal(0, 1), class = sigma),
                prior(normal(0, 1), class = sd),
                prior(normal(0, .1), class = b),
                prior(lkj(2), class = cor)),
      iter = 3000)
toc()
```

We visualize the posterior distributions and model diagnostics:

```{r}
plot(fit_surp)
```


The effect of surprisal (in bits) on log-RT is summarized below:

```{r}
posterior_summary(fit_surp, variable = "b_surprisal")
```

The conditional effects of surprisal on the original millisecond scale can be plotted as well: 

```{r}
conditional_effects(fit_surp)
```

# Conclusion

This vignette demonstrates how to calculate surprisal values using the `pangoling` package and estimate their effect on reading times using Bayesian hierarchical modeling. By leveraging pre-trained transformer models, researchers can explore the cognitive effects of surprisal in language comprehension experiments.

@levy2008 theory highlights how surprisal acts as a cognitive bottleneck: higher surprisal reflects greater unexpectedness, which slows down processing due to the reallocation of cognitive resources. This example illustrates how models like GPT-2 can operationalize surprisal and facilitate testing such theories in empirical data.


# References

