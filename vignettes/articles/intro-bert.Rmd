---
title: "Using a Bert model to get the predictability of words in their context"
bibliography: '`r system.file("REFERENCES.bib", package="pangoling")`'
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


<!-- https://ropensci.org/blog/2019/12/08/precompute-vignettes/ -->

Whereas [the vignette about GPT-2](intro-gpt2.html) presents a very popular way 
to calculate word probabilities using GPT-like models, masked models present an
alternative, especially, when we just care about the final word following a 
certain context.

A masked language model (also called BERT-like, or encoder model) is a type of 
large language model  that can be used to predict the content of a mask in a 
sentence. BERT is an example of a masked language model 
[see also @Devlinetal2018].


First load the following packages:

```{r setup, message = FALSE}
library(pangoling)
library(tidytable) # fast alternative to dplyr
```


Notice the following potential pitfall. This would be a **bad** approach for
making predictions in a masked model:

```{r}
masked_tokens_pred_tbl("The apple doesn't fall far from the [MASK]")
```
(The pretrained models and tokenizers will be downloaded from 
https://huggingface.co/ the first time they are used.)


The most common predictions are punctuation marks, because BERT uses the left 
*and* right context. In this case, the right context indicates that the mask is 
the final *token* of the sentence.
More expected results are obtained in the following way:

```{r}
masked_tokens_pred_tbl("The apple doesn't fall far from the [MASK].")
```

We can mask several tokens as well (but bear in mind that this type of models 
are trained with only 10-15% of masks):

```{r}
df_masks <- 
  masked_tokens_pred_tbl("The apple doesn't fall far from the [MASK][MASK]")
df_masks |> filter(mask_n == 1)
df_masks |> filter(mask_n == 2)
```

We can also use BERT to examine the predictability of words assuming that both 
the left and right contexts are known:

```{r}
(df_sent <- data.frame(
  left = c("The", "The"),
  critical = c("apple", "pear"),
  right = c(
    "doesn't fall far from the tree.",
    "doesn't fall far from the tree."
  )
))
```

The function `masked_targets_pred()` will give us the log-probability of the 
target word (and will take care of summing the log-probabilities in case the 
target is composed by several tokens).

```{r}
df_sent <- df_sent %>%
  mutate(lp = masked_targets_pred(
    prev_contexts = left,
    targets = critical,
    after_contexts = right
  ))
df_sent
```

As expected (given the popularity of the proverb), "apple" is a more likely 
target word than "pear".


# References
