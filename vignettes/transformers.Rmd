---
title: "Using transformer models to get word predictability."
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{transformers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(pangoling)
library(tidytable)
library(tictoc)
```

```{r, cache = TRUE}
data_natural_stim <- data_natural_spr |> 
  distinct.(item, word_n, word)
```

```{r, cache = TRUE}
data_natural_stim |>
  summarize.(item_text = paste0(word, collapse =" "), 
             nwords = length(word),
             .by = "item") |>
  mutate.(ntokens_gpt2 = ntokens(item_text, model = "gpt2"),
          .by = "item")

```

```{r, cache = TRUE}
tic()
data_natural_stim <- data_natural_stim |>
  mutate.(lp_gpt2 = get_causal_log_prob(word, 
                                                 stride = 100,
                                                 model = "gpt2",
                                                 by = item))
toc()
saveRDS(data_natural_stim,file="data_natural_stim.RDS")
```


```{r}
data_natural_stim <- data_natural_stim |> mutate.(lp_gpt2 = unlist(lp_gpt2))
# default = 1
data_natural_stim <- data_natural_stim |> mutate.(fpmw = get_word_fpmw(word)) |>
  mutate.(log_fpmw = log(fpmw),
          wl = nchar(word))

data_natural_spr <- left_join.(data_natural_spr, data_natural_stim)
```
```{r}
library(lme4)
summary(m <- lmer(log(RT) ~ scale(lp_gpt2) + scale(log_fpmw) + scale(wl)  +(scale(lp_gpt2) + scale(log_fpmw) + scale(wl)||subj), data = data_natural_spr |> filter.(RT > 100)))
```

```{r}

data_natural_stim <- data_natural_stim |>
  mutate.(lp_gpt2 = get_masked_log_prob(word, 
                                            #     stride = 100,
                                                 model = "bert-base-uncased",
                                                 by = item))
```

$p_affected = logit^{-1}({affected} + trial_n {trial})\ {affected} Beta(8,2)\ {trial} Normal(0, .1)$$
