% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tr_causal.R
\name{causal_words_pred}
\alias{causal_words_pred}
\alias{causal_tokens_pred_lst}
\alias{causal_targets_pred}
\title{Compute predictability using a causal transformer model}
\usage{
causal_words_pred(
  x,
  by = rep(1, length(x)),
  sep = " ",
  log.p = getOption("pangoling.log.p"),
  ignore_regex = "",
  model = getOption("pangoling.causal.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL,
  batch_size = 1,
  ...
)

causal_tokens_pred_lst(
  texts,
  log.p = getOption("pangoling.log.p"),
  model = getOption("pangoling.causal.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL,
  batch_size = 1
)

causal_targets_pred(
  targets,
  contexts = NULL,
  sep = " ",
  log.p = getOption("pangoling.log.p"),
  ignore_regex = "",
  model = getOption("pangoling.causal.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL,
  batch_size = 1,
  ...
)
}
\arguments{
\item{x}{A character vector of words, phrases, or texts to evaluate (for \code{causal_words_pred()}).}

\item{by}{A grouping variable indicating how texts are split into groups (for \code{causal_words_pred()}).}

\item{sep}{A string specifying how words are separated within contexts or groups. Default is \code{" "}. For languages that don't have spaces between words (e.g., Chinese), set \code{sep = ""}.}

\item{log.p}{Base of the logarithm used for the output predictability values.
If \code{TRUE} (default), the natural logarithm (base \emph{e}) is used.
If \code{FALSE}, the raw probabilities are returned.
Alternatively, \code{log.p} can be set to a numeric value specifying
the base of the logarithm (e.g., \code{2} for base-2 logarithms).
To get surprisal in bits (rather than predictability), set
\code{log.p = 1/2}.}

\item{ignore_regex}{Can ignore certain characters when calculating the log
probabilities. For example \verb{^[[:punct:]]$} will ignore
all punctuation  that stands alone in a token.}

\item{model}{Name of a pre-trained model or folder. One should be able to use
models based on "gpt2". See
\href{https://huggingface.co/models?other=gpt2}{hugging face website}.}

\item{checkpoint}{Folder of a checkpoint.}

\item{add_special_tokens}{Whether to include special tokens. It has the
same default as the
\href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoTokenizer}{AutoTokenizer}
method in Python.}

\item{config_model}{List with other arguments that control how the
model from Hugging Face is accessed.}

\item{config_tokenizer}{List with other arguments that control how the
tokenizer from Hugging Face is accessed.}

\item{batch_size}{Maximum size of the batch. Larger batches speedup
processing but take more memory.}

\item{...}{Currently not in use.}

\item{texts}{A vector or list of sentences or paragraphs (for \code{causal_tokens_pred_lst()}).}

\item{targets}{A character vector of target words or phrases (for \code{causal_targets_pred()}).}

\item{contexts}{A character vector of contexts corresponding to each target (for \code{causal_targets_pred()}).}
}
\value{
For \code{causal_targets_pred()} and \code{causal_words_pred()},
a named numeric vector of predictability scores. For
\code{causal_tokens_pred_lst()}, a list of named numeric vectors, one for
each sentence or group.
}
\description{
These functions calculate the predictability of words or phrases using a
causal transformer model.
Depending on the use case, the following functions are available:
}
\details{
\itemize{
\item \strong{\code{causal_targets_pred()}}: Evaluates specific target words or phrases
based on their given contexts.
\item \strong{\code{causal_words_pred()}}: Computes predictability for all elements of a
vector grouped by a specified variable.
\item \strong{\code{causal_tokens_pred_lst()}}: Computes the predictability of each token in a sentence (or group of sentences) and returns a list of results for each sentence.
}

These functions calculate the predictability (by default the natural
logarithm of the word probability) of words or phrases using a causal
transformer model:
\itemize{
\item Use \code{causal_targets_pred()} when you have explicit context-target pairs to evaluate, with each target word or phrase paired with a single preceding context.
\item Use \code{causal_words_pred()} when working with entire texts split into groups, such as sentences, where predictability is computed for every word or phrase in each group.
\item Use \code{causal_tokens_pred_lst()} when you want to calculate the predictability of every token in one or more sentences, returning results as a list that can be converted into a data frame if needed.
}

See the
\href{https://bruno.nicenboim.me/pangoling/articles/intro-gpt2.html}{online article}
in pangoling website for more examples.
}
\section{More examples}{

See the
\href{https://bruno.nicenboim.me/pangoling/articles/intro-gpt2.html}{online article}
in pangoling website for more examples.
}

\examples{
# Using causal_targets_pred
causal_targets_pred(
  targets = c("tree.", "cover."),
  contexts = c("The apple doesn't fall far from the",
               "Don't judge a book by its"),
  model = "gpt2"
)

# Using causal_words_pred
causal_words_pred(
  x = df_sent$word,
  by = df_sent$sent_n,
  model = "gpt2"
)

# Using causal_tokens_pred_lst
preds <- causal_tokens_pred_lst(
  texts = c("The apple doesn't fall far from the tree.",
            "Don't judge a book by its cover."),
  model = "gpt2"
)
# Convert the output to a tidy table
library(tidytable)
map2_dfr(preds, seq_along(preds), 
~ data.frame(tokens = names(.x), pred = .x, id = .y))

}
\seealso{
Other causal model functions: 
\code{\link{causal_next_tokens_pred_tbl}()},
\code{\link{causal_pred_mats}()}
}
\concept{causal model functions}
