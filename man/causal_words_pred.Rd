% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tr_causal.R
\name{causal_words_pred}
\alias{causal_words_pred}
\title{Get the predictability of each element of a vector of words (or phrases) in a
series of texts using a causal transformer}
\usage{
causal_words_pred(
  x,
  by = rep(1, length(x)),
  sep = " ",
  log.p = getOption("pangoling.log.p"),
  ignore_regex = "",
  model = getOption("pangoling.causal.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL,
  batch_size = 1,
  ...
)
}
\arguments{
\item{x}{Vector of (non-empty) words, phrases or texts.}

\item{by}{Vector that indicates how the text should be split. By default,
this is \code{" "}, but for languages, such as Chinese, which don't
separate words, this should be set to \code{""}.}

\item{sep}{Character indicating how words are separated in a sentence.}

\item{log.p}{Base of the logarithm used for the output predictability values.
If \code{TRUE} (default), the natural logarithm (base \emph{e}) is used.
If \code{FALSE}, the raw probabilities are returned.
Alternatively, \code{log.p} can be set to a numeric value specifying
the base of the logarithm (e.g., \code{2} for base-2 logarithms).
To get surprisal in bits (rather than predictability), set
\code{log.p = 1/2}.}

\item{ignore_regex}{Can ignore certain characters when calculates the log
probabilities. For example \verb{^[[:punct:]]$} will ignore
all punctuation  that stands alone in a token.}

\item{model}{Name of a pre-trained model or folder.}

\item{checkpoint}{Folder of a checkpoint.}

\item{add_special_tokens}{Whether to include special tokens. It has the
same default as the
\href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoTokenizer}{AutoTokenizer}
method in Python.}

\item{config_model}{List with other arguments that control how the
model from Hugging Face is accessed.}

\item{config_tokenizer}{List with other arguments that control how the
tokenizer from Hugging Face is accessed.}

\item{batch_size}{Maximum size of the batch. Larges batches speedup
processing but take more memory.}

\item{...}{not in use.}
}
\value{
A named vector of (log) probabilities.
}
\description{
Get the predictability (by default the natural logarithm of the word
probability) of each element of a vector of words (or phrases) in a series of
texts using a causal transformer model. See the
\link{online article}
(https://bruno.nicenboim.me/pangoling/articles/intro-gpt2.html)
in pangoling website for more examples.
}
\details{
A causal language model (also called GPT-like, auto-regressive, or decoder
model) is a type of large language model usually used for text-generation
that can predict the next word (or more accurately in fact token) based
on a preceding context.

If not specified, the causal model that will be used is the one set in
specified in the global option \code{pangoling.causal.default}, this can be
accessed via \code{getOption("pangoling.causal.default")} (by default
"gpt2"). To change the default option
use \code{options(pangoling.causal.default = "newcausalmodel")}.

A list of possible causal models can be found in
\href{https://huggingface.co/models?pipeline_tag=text-generation}{Hugging Face website}.

Using the  \code{config_model} and \code{config_tokenizer} arguments, it's possible to
control how the model and tokenizer from Hugging Face is accessed, see the
Python method
\href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoProcessor.from_pretrained}{\code{from_pretrained}}
for details.

In case of errors when a new model is run, check the status of
\url{https://status.huggingface.co/}
}
\section{More examples}{

See the
\href{https://bruno.nicenboim.me/pangoling/articles/intro-gpt2.html}{online article}
in pangoling website for more examples.
}

\examples{
\dontshow{if (interactive()) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
example_data <- tribble(
   ~sent_n, ~word,
       1,  "The",
       1,  "apple",
       1,  "doesn't",
       1,  "fall",
       1,  "far",
       1,  "from",
       1,  "the",
       1,  "tree.",
       2,  "Don't",
       2,  "judge",
       2,  "a",
       2,  "book",
       2,  "by",
       2,  "its",
       2,  "cover."
)
causal_words_pred(
  x = example_data$word,
  by = example_data$sent_n,
  model = "gpt2"
)
causal_words_pred(
  x = example_data$word,
  by = example_data$sent_n,
  model = "gpt2",
  # surprisal values in bits (-log2(prob) = log(prob, base = 1/2))
  log.p = 1/2  
)
\dontshow{\}) # examplesIf}
}
\seealso{
Other causal model functions: 
\code{\link{causal_next_tokens_pred_tbl}()},
\code{\link{causal_pred_mats}()},
\code{\link{causal_targets_pred}()},
\code{\link{causal_tokens_pred_tbl}()}
}
\concept{causal model functions}
