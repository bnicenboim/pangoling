% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tr_masked.R
\name{masked_tokens_pred_tbl}
\alias{masked_tokens_pred_tbl}
\title{Get the possible tokens and their log probabilities for each mask in a
sentence}
\usage{
masked_tokens_pred_tbl(
  masked_sentences,
  log.p = getOption("pangoling.log.p"),
  model = getOption("pangoling.masked.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL
)
}
\arguments{
\item{masked_sentences}{Masked sentences.}

\item{log.p}{Base of the logarithm used for the output predictability values.
If \code{TRUE} (default), the natural logarithm (base \emph{e}) is used.
If \code{FALSE}, the raw probabilities are returned.
Alternatively, \code{log.p} can be set to a numeric value specifying
the base of the logarithm (e.g., \code{2} for base-2 logarithms).
To get surprisal in bits (rather than predictability), set
\code{log.p = 1/2}.}

\item{model}{Name of a pre-trained model or folder. One should be able to use
models based on "bert". See
\href{https://huggingface.co/models?other=bert}{hugging face website}.}

\item{checkpoint}{Folder of a checkpoint.}

\item{add_special_tokens}{Whether to include special tokens. It has the
same default as the
\href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoTokenizer}{AutoTokenizer}
method in Python.}

\item{config_model}{List with other arguments that control how the
model from Hugging Face is accessed.}

\item{config_tokenizer}{List with other arguments that control how the
tokenizer from Hugging Face is accessed.}
}
\value{
A table with the masked sentences, the tokens (\code{token}),
predictability (\code{pred}), and the respective mask number (\code{mask_n}).
}
\description{
For each mask, indicated with \verb{[MASK]}, in a sentence, get the possible
tokens and their  predictability (by default the natural logarithm of the
word probability) using a masked transformer.
}
\details{
A masked language model (also called BERT-like, or encoder model) is a type
of large language model  that can be used to predict the content of a mask
in a sentence.

If not specified, the masked model that will be used is the one set in
specified in the global option \code{pangoling.masked.default}, this can be
accessed via \code{getOption("pangoling.masked.default")} (by default
"bert-base-uncased"). To change the default option
use \code{options(pangoling.masked.default = "newmaskedmodel")}.

A list of possible masked can be found in
\href{https://huggingface.co/models?pipeline_tag=fill-mask}{Hugging Face website}

Using the  \code{config_model} and \code{config_tokenizer} arguments, it's possible to
control how the model and tokenizer from Hugging Face is accessed, see the
python method
\href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoProcessor.from_pretrained}{\code{from_pretrained}}
for details. In case of errors check the status of
\url{https://status.huggingface.co/}
}
\section{More examples}{

See the
\href{https://bruno.nicenboim.me/pangoling/articles/intro-bert.html}{online article}
in pangoling website for more examples.
}

\examples{
masked_tokens_pred_tbl("The [MASK] doesn't fall far from the tree.",
  model = "bert-base-uncased"
)

}
\seealso{
Other masked model functions: 
\code{\link{masked_targets_pred}()}
}
\concept{masked model functions}
