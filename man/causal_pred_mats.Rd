% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tr_causal.R
\name{causal_pred_mats}
\alias{causal_pred_mats}
\title{Generate a list of predictability matrices using a causal transformer model}
\usage{
causal_pred_mats(
  x,
  by = rep(1, length(x)),
  sep = " ",
  log.p = getOption("pangoling.log.p"),
  sorted = FALSE,
  model = getOption("pangoling.causal.default"),
  checkpoint = NULL,
  add_special_tokens = NULL,
  decode = FALSE,
  config_model = NULL,
  config_tokenizer = NULL,
  batch_size = 1,
  ...
)
}
\arguments{
\item{x}{Vector of (non-empty) words, phrases or texts.}

\item{by}{Vector that indicates how the text should be split. By default,
this is \code{" "}, but for languages, such as Chinese, which don't
separate words, this should be set to \code{""}.}

\item{sep}{Character indicating how words are separated in a sentence.}

\item{log.p}{Base of the logarithm used for the output predictability values.
If \code{TRUE} (default), the natural logarithm (base \emph{e}) is used.
If \code{FALSE}, the raw probabilities are returned.
Alternatively, \code{log.p} can be set to a numeric value specifying
the base of the logarithm (e.g., \code{2} for base-2 logarithms).
To get surprisal in bits (rather than predictability), set
\code{log.p = 1/2}.}

\item{sorted}{When default FALSE it will retain the order of groups we are
splitting on. When TRUE then sorted (according to \code{by}) list(s)
are returned.}

\item{model}{Name of a pre-trained model or folder. One should be able to use
models based on "gpt2". See
\href{https://huggingface.co/models?other=gpt2}{hugging face website}.}

\item{checkpoint}{Folder of a checkpoint.}

\item{add_special_tokens}{Whether to include special tokens. It has the
same default as the
\href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoTokenizer}{AutoTokenizer}
method in Python.}

\item{decode}{Should it decode the tokens into readable strings? This is
relevant for special characters such as accents and
diacritics, which get mangled in the tokens.}

\item{config_model}{List with other arguments that control how the
model from Hugging Face is accessed.}

\item{config_tokenizer}{List with other arguments that control how the
tokenizer from Hugging Face is accessed.}

\item{batch_size}{Maximum size of the batch. Larges batches speedup
processing but take more memory.}

\item{...}{not in use.}
}
\value{
A list of matrices with tokens in their columns and the vocabulary of
the model in their rows
}
\description{
This function computes a list of matrices, where each matrix corresponds to a
unique group specified by the \code{by} argument. Each matrix represents the
predictability of every token in the input text (\code{x}) based on preceding
context, as evaluated by a causal transformer model.
The predictability values are given as the natural logarithm of word
probabilities by default.
}
\details{
Each matrix contains:
\itemize{
\item Rows representing the model's vocabulary.
\item Columns corresponding to tokens in the group (e.g., a sentence or
paragraph).
}

The function splits the input \code{x} into groups specified by the \code{by} argument
and processes each group independently. For each group, the causal
transformer model computes the predictability of every token in the model's
vocabulary, conditioned on the context leading up to that token.
}
\section{More examples}{

See the
\href{https://bruno.nicenboim.me/pangoling/articles/intro-gpt2.html}{online article}
in pangoling website for more examples.
}

\examples{
list_of_mats <- causal_pred_mats(
                       x = df_sent$word,
                       by = df_sent$sent_n,  
                       model = "gpt2"
                )
list_of_mats |> str()
list_of_mats[[1]] |> tail()
list_of_mats[[2]] |> tail()
}
\seealso{
Other causal model functions: 
\code{\link{causal_next_tokens_pred_tbl}()},
\code{\link{causal_targets_pred}()},
\code{\link{causal_tokens_pred_tbl}()},
\code{\link{causal_words_pred}()}
}
\concept{causal model functions}
