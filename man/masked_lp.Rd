% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tr_masked.R
\name{masked_lp}
\alias{masked_lp}
\title{Get the log probability of the last word (or phrase) of given a context}
\usage{
masked_lp(
  l_contexts,
  targets,
  r_contexts,
  ignore_regex = "",
  model = getOption("pangoling.masked.default"),
  add_special_tokens = NULL,
  config_model = NULL,
  config_tokenizer = NULL
)
}
\arguments{
\item{l_contexts}{Left context of the target word.}

\item{targets}{Target words.}

\item{r_contexts}{Right context of the target word.}

\item{ignore_regex}{Can ignore certain characters when calculates the log probabilities. For example \verb{^[[:punct:]]$} will ignore all punctuation  that stands alone in a token.}

\item{model}{Name of a pre-trained model.}

\item{add_special_tokens}{Whether to include special tokens. It has the same default as the \href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoTokenizer}{AutoTokenizer} method in Python.}

\item{config_model}{List with other arguments that control how the model from Hugging Face is accessed.}

\item{config_tokenizer}{List with other arguments that control how the tokenizer from Hugging Face is accessed.}
}
\value{
A named vector of log probabilities.
}
\description{
Get the log probability of the last word (or phrase) of given a context using
a masked transformer
}
\details{
A masked language model (also called BERT-like, or encoder model) is a type
of large language model  that can be used to predict the content of a mask
in a sentence.

If not specified, the causal model that will be used is the one set in
specified in the global option \code{pangoling.masked.default}, this can be
accessed via \code{getOption("pangoling.masked.default")} (by default
"bert-base-uncased"). To change the default option
use \code{options(pangoling.masked.default = "newmaskedmodel")}.

A list of possible causal masked can be found in
\href{https://huggingface.co/}{Hugging Face website}.

Using the  \code{config_model} and \code{config_tokenizer} arguments, it's possible to
control how the model and tokenizer from Hugging Face is accessed, see the
python method
\href{https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/auto#transformers.AutoProcessor.from_pretrained}{\code{from_pretrained}} for details. In case of errors
check the status of \url{https://status.huggingface.co/}
}
\section{More examples}{

See the  \href{https://bruno.nicenboim.me/pangoling/articles/intro-bert.html}{online article} in pangoling website for more examples.
}

\examples{
\dontshow{if (interactive()) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
masked_lp(
  l_contexts = c("The", "The"),
  targets = c("apple", "pear"),
  r_contexts = c(
    "doesn't fall far from the tree.",
    "doesn't fall far from the tree."
  ),
  model = "bert-base-uncased"
)
\dontshow{\}) # examplesIf}
}
\seealso{
Other masked model functions: 
\code{\link{masked_config}()},
\code{\link{masked_preload}()},
\code{\link{masked_tokens_tbl}()}
}
\concept{masked model functions}
