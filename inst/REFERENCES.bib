@Article{Frank2015,
  author       = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  date         = {2015-01},
  journaltitle = {Brain and Language},
  title        = {The {{ERP}} Response to the Amount of Information Conveyed by Words in Sentences},
  doi          = {10.1016/j.bandl.2014.10.006},
  issn         = {0093-934X},
  pages        = {1--11},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0093934X14001515},
  volume       = {140},
  abstract     = {Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence’s hierarchical structure for generating expectations about the upcoming word. Ó 2014 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/3.0/).},
  langid       = {english}
}
@Article{Frank2013,
  author       = {Frank, Stefan L. and Fernandez Monsalve, Irene and Thompson, Robin L. and Vigliocco, Gabriella},
  date         = {2013-12-01},
  journaltitle = {Behavior Research Methods},
  title        = {Reading Time Data for Evaluating Broad-Coverage Models of {{English}} Sentence Processing},
  doi          = {10.3758/s13428-012-0313-y},
  issn         = {1554-3528},
  number       = {4},
  pages        = {1182--1190},
  volume       = {45},
  abstract     = {We make available word-by-word self-paced reading times and eye-tracking data over a sample of English sentences from narrative sources. These data are intended to form a gold standard for the evaluation of computational psycholinguistic models of sentence comprehension in English. We describe stimuli selection and data collection and present descriptive statistics, as well as comparisons between the two sets of reading times.},
  keywords     = {Eye tracking, Model evaluation, Self-paced reading, Sentence comprehension, Word-reading time},
  langid       = {english},
  shortjournal = {Behav Res},
}
@Article{Frank2013a,
  author       = {Frank, Stefan L.},
  date         = {2013},
  journaltitle = {Topics in Cognitive Science},
  title        = {Uncertainty {{Reduction}} as a {{Measure}} of {{Cognitive Load}} in {{Sentence Comprehension}}},
  doi          = {10.1111/tops.12025},
  issn         = {1756-8765},
  number       = {3},
  pages        = {475--494},
  volume       = {5},
  abstract     = {The entropy-reduction hypothesis claims that the cognitive processing difficulty on a word in sentence context is determined by the word's effect on the uncertainty about the sentence. Here, this hypothesis is tested more thoroughly than has been done before, using a recurrent neural network for estimating entropy and self-paced reading for obtaining measures of cognitive processing load. Results show a positive relation between reading time on a word and the reduction in entropy due to processing that word, supporting the entropy-reduction hypothesis. Although this effect is independent from the effect of word surprisal, we find no evidence that these two measures correspond to cognitively distinct processes.},
  keywords     = {Cognitive load, Entropy reduction, Recurrent neural network, Self-paced reading, Sentence comprehension, Surprisal, Word information},
  langid       = {english},
}

@article{provo,
  title={The {Provo} Corpus: {A} large eye-tracking corpus with predictability norms},
  author={Luke, Steven G and Christianson, Kiel},
  journal={Behavior research methods},
  volume={50},
  pages={826--833},
  year={2018},
  publisher={Springer}
}


@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: {State}-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{Devlinetal2018,
  doi = {10.48550/ARXIV.1810.04805},

  url = {https://arxiv.org/abs/1810.04805},

  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},

  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {{BERT:} {Pre}-training of Deep Bidirectional Transformers for Language Understanding},

  publisher = {arXiv},

  year = {2018},

  copyright = {arXiv.org perpetual, non-exclusive license}
}


