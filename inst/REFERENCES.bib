@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: {State}-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{Devlinetal2018,
  doi = {10.48550/ARXIV.1810.04805},

  url = {https://arxiv.org/abs/1810.04805},

  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},

  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {{BERT:} {Pre}-training of Deep Bidirectional Transformers for Language Understanding},

  publisher = {arXiv},

  year = {2018},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{JaegerEtAl2015,
title = {The subject-relative advantage in Chinese: Evidence for expectation-based processing},
journal = {Journal of Memory and Language},
volume = {79-80},
pages = {97-120},
year = {2015},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2014.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X14001272},
author = {Lena Jäger and Zhong Chen and Qiang Li and Chien-Jer Charles Lin and Shravan Vasishth},
keywords = {Sentence processing, Relative clause, Structural expectation, Working-memory, Surprisal, Chinese},
abstract = {Chinese relative clauses are an important test case for pitting the predictions of expectation-based accounts against those of memory-based theories. The memory-based accounts predict that object relatives are easier to process than subject relatives because, in object relatives, the distance between the relative clause verb and the head noun is shorter. By contrast, expectation-based accounts such as surprisal predict that the less frequent object relative should be harder to process. In previous studies on Chinese relative clause comprehension, local ambiguities may have rendered a comparison between relative clause types uninterpretable. We designed experimental materials in which no local ambiguities confound the comparison. We ran two experiments (self-paced reading and eye-tracking) to compare reading difficulty in subject and object relatives which were placed either in subject or object modifying position. The evidence from our studies is consistent with the predictions of expectation-based accounts but not with those of memory-based theories.}
}
@article{levy2008,
  author       = {Roger Levy},
  title        = {Expectation-based syntactic comprehension},
  journal      = {Cognition},
  volume       = {106},
  number       = {3},
  pages        = {1126--1177},
  year         = {2008},
  doi          = {10.1016/j.cognition.2007.05.006}
}
