<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading • pangoling</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading">
<script>if(!sessionStorage.getItem("_swa")&&document.referrer.indexOf(location.protocol+"//"+location.host)!== 0){fetch("https://counter.dev/track?"+new URLSearchParams({referrer:document.referrer,screen:screen.width+"x"+screen.height,user:"bnicenboim",utcoffset:"1"}))};sessionStorage.setItem("_swa","1");</script>
</head>
<body>
<p>,<script data-goatcounter="https://bnicenboim.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script></p>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">pangoling</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9010</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/intro-gpt2.html">Using a GPT2 transformer model to get word predictability</a></li>
    <li><a class="dropdown-item" href="../articles/intro-bert.html">Using a Bert model to get the predictability of words in their context</a></li>
    <li><a class="dropdown-item" href="../articles/example.html">Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading</a></li>
    <li><a class="dropdown-item" href="../articles/troubleshooting.html">Troubleshooting the use of Python in R</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/bnicenboim/pangoling/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/bnicenboim/pangoling/blob/main/vignettes/articles/example.Rmd" class="external-link"><code>vignettes/articles/example.Rmd</code></a></small>
      <div class="d-none name"><code>example.Rmd</code></div>
    </div>

    
    
<p>This vignette demonstrates how to use the <code>pangoling</code>
package to estimate the effect of <em>surprisal</em> on reading times
(RTs) in a psycholinguistic experiment.</p>
<p>Surprisal is a measure of processing difficulty based on the
unexpectedness of a word in its context. In information theory, the
surprisal <span class="citation">(or Shannon’s Information, see Shannon
1948)</span> of x in bits is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\log_2 P(x)</annotation></semantics></math>.
In the context of language, this translates to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">word</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="normal">context</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\log_2 P(\text{word}|\text{context})</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">word</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="normal">context</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\text{word}|\text{context})</annotation></semantics></math>
is the probability of the word given its preceding context. Higher
surprisal values indicate greater processing difficulty and suggest a
cognitive processing bottleneck, as readers allocate more resources to
processing unexpected words <span class="citation">(Levy 2008; Hale
2001)</span>. When surprisal is high, cognitive processing is supposed
to slow down, which can be observed through longer reading times in
reading tasks.</p>
<p>Surprisal is computed here using a GPT-2-like causal language model
trained on Chinese texts. We use <code>pangoling</code> to retrieve the
surprisal values. The dataset comes from <span class="citation">Jäger et
al. (2015)</span>, which features Chinese sentences presented
word-by-word to participants during a self-paced reading experiment.</p>
<div class="section level2">
<h2 id="data-analysis">Data analysis<a class="anchor" aria-label="anchor" href="#data-analysis"></a>
</h2>
<div class="section level3">
<h3 id="preprocessing">0. Preprocessing<a class="anchor" aria-label="anchor" href="#preprocessing"></a>
</h3>
<p>We load the required libraries first.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://bruno.nicenboim.me/pangoling" class="external-link">pangoling</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://markfairbanks.github.io/tidytable/" class="external-link">tidytable</a></span><span class="op">)</span> <span class="co"># Lightweight and faster alternative to dplyr</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/jabiru/tictoc" class="external-link">tictoc</a></span><span class="op">)</span>  <span class="co"># Report the time that a piece of code takes</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/paul-buerkner/brms" class="external-link">brms</a></span><span class="op">)</span> <span class="co"># Bayesian regression models in Stan</span></span>
<span><span class="co"># Use all the cores needed available for brms:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>mc.cores <span class="op">=</span> <span class="fu">parallel</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/parallel/detectCores.html" class="external-link">detectCores</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<p>The dataset <code>df_jaeger14</code> from <span class="citation">Jäger et al. (2015)</span> contains Chinese sentences
presented word-by-word to participants. Each word has an associated
reaction time (RT). (The original research question and the experimental
conditions don’t matter to us.)</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"df_jaeger14"</span><span class="op">)</span></span>
<span><span class="va">df_jaeger14</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tidytable: 8,624 × 14</span></span></span>
<span><span class="co">#&gt;    subject  item cond  word  wordn    rt region question accuracy correct_answer</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;fct&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       <span style="color: #949494; font-style: italic;">&lt;int&gt;</span>          <span style="color: #949494; font-style: italic;">&lt;int&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span> 1m1         1 a     那個      1   360 Det+CL 那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span> 1m1         1 a     昨晚      2   359 Adv    那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span> 1m1         1 a     揍了服務…     3   344 VN     那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span> 1m1         1 a     一頓      4   313 FreqP  那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span> 1m1         1 a     的        5   297 DE     那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span> 1m1         1 a     顧客      6   312 head   那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span> 1m1         1 a     聽說過……     7   297 hd1    那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span> 1m1         1 a     老闆      8   281 hd2    那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span> 1m1         1 a     並且      9   297 hd3    那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span> 1m1         1 a     記得     10   313 hd4    那個顧客聽說過…        1              1</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 8,614 more rows</span></span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 4 more variables: question_type &lt;int&gt;, experiment &lt;chr&gt;, list &lt;int&gt;,</span></span></span>
<span><span class="co">#&gt; <span style="color: #949494;">#   sentence &lt;chr&gt;</span></span></span></code></pre></div>
<p>To avoid redundant computations, we extract unique combinations of
items, conditions, and words. (We don’t want to run our functions on the
entire dataset because there will be repetition across subjects, and
thus the model will run several times on identical words in identical
sentences):</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df_stim</span> <span class="op">&lt;-</span> <span class="va">df_jaeger14</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://markfairbanks.github.io/tidytable/reference/distinct.html" class="external-link">distinct</a></span><span class="op">(</span><span class="va">item</span>, <span class="va">cond</span>, <span class="va">word</span>, <span class="va">wordn</span><span class="op">)</span></span>
<span><span class="va">df_stim</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tidytable: 704 × 4</span></span></span>
<span><span class="co">#&gt;     item cond  word       wordn</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;int&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span>     1 a     那個           1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span>     1 a     昨晚           2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span>     1 a     揍了服務生     3</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span>     1 a     一頓           4</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span>     1 a     的             5</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span>     1 a     顧客           6</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span>     1 a     聽說過         7</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span>     1 a     老闆           8</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span>     1 a     並且           9</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span>     1 a     記得          10</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">11</span>     1 a     他。          11</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">12</span>     2 b     那輛           1</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">13</span>     2 b     下午           2</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">14</span>     2 b     摩托車追了     3</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">15</span>     2 b     很久           4</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">16</span>     2 b     的             5</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">17</span>     2 b     轎車           6</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">18</span>     2 b     發現了         7</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">19</span>     2 b     記者           8</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">20</span>     2 b     所以           9</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 684 more rows</span></span></span></code></pre></div>
<p>We use the <code>pangoling</code> function
<code><a href="../reference/causal_preload.html">causal_preload()</a></code> to load the GPT-2 model <a href="https://huggingface.co/uer/gpt2-chinese-cluecorpussmall" class="external-link"><code>uer/gpt2-chinese-cluecorpussmall</code></a>.
This model is trained on Chinese texts and can predict word
probabilities based on preceding context.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/causal_preload.html">causal_preload</a></span><span class="op">(</span><span class="st">"uer/gpt2-chinese-cluecorpussmall"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Preloading causal model uer/gpt2-chinese-cluecorpussmall...</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="add-surprisal-values-to-the-dataset">1. Add surprisal values to the dataset<a class="anchor" aria-label="anchor" href="#add-surprisal-values-to-the-dataset"></a>
</h3>
<p>Surprisal in bits (i.e., we use a logarithm with base 2) is
calculated as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">word</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="normal">context</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\log_2(P(\text{word}|\text{context}))</annotation></semantics></math>,
which is equivalent to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>log</mo><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">word</mtext><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="normal">context</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log_{1/2}(P(\text{word}|\text{context}))</annotation></semantics></math>.
The <code>pangoling</code> function <code><a href="../reference/causal_predictability.html">causal_words_pred()</a></code> is
configured to:</p>
<ul>
<li>Use <code>log.p = 0.5</code> to get surprisal in bits.</li>
<li>Set <code>sep = ""</code> since Chinese words are not separated by
spaces.</li>
<li>Process sentences in batches of 10 for efficiency.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html" class="external-link">tic</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">df_stim</span> <span class="op">&lt;-</span> <span class="va">df_stim</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://markfairbanks.github.io/tidytable/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>surprisal <span class="op">=</span> <span class="fu"><a href="../reference/causal_predictability.html">causal_words_pred</a></span><span class="op">(</span></span>
<span>    <span class="va">word</span>,</span>
<span>    by <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/interaction.html" class="external-link">interaction</a></span><span class="op">(</span><span class="va">item</span>, <span class="va">cond</span><span class="op">)</span>,</span>
<span>    log.p <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>    model <span class="op">=</span> <span class="st">"uer/gpt2-chinese-cluecorpussmall"</span>,</span>
<span>    sep <span class="op">=</span> <span class="st">""</span>,</span>
<span>    batch_size <span class="op">=</span> <span class="fl">10</span></span>
<span>  <span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Processing using causal model 'uer/gpt2-chinese-cluecorpussmall/' ...</span></span>
<span><span class="co">#&gt; Processing a batch of size 10 with 33 tokens.</span></span>
<span><span class="co">#&gt; Processing a batch of size 10 with 33 tokens.</span></span>
<span><span class="co">#&gt; Processing a batch of size 10 with 33 tokens.</span></span>
<span><span class="co">#&gt; Processing a batch of size 10 with 33 tokens.</span></span>
<span><span class="co">#&gt; Processing a batch of size 10 with 33 tokens.</span></span>
<span><span class="co">#&gt; Processing a batch of size 10 with 33 tokens.</span></span>
<span><span class="co">#&gt; Processing a batch of size 4 with 33 tokens.</span></span>
<span><span class="co">#&gt; Text id: 1.a</span></span>
<span><span class="co">#&gt; `那個昨晚揍了服務生一頓的顧客聽說過老闆並且記得他。`</span></span>
<span><span class="co">#&gt; Text id: 2.a</span></span>
<span><span class="co">#&gt; `那輛下午追了摩托車很久的轎車發現了記者所以停了下來。`</span></span>
<span><span class="co">#&gt; Text id: 3.a</span></span>
<span><span class="co">#&gt; `那個今天打了男孩幾次的女孩看到了校長所以假裝讀書。`</span></span>
<span><span class="co">#&gt; Text id: 4.a</span></span>
<span><span class="co">#&gt; `那輛當時撞了自行車兩次的吉普車攔住了警察並且要求調查清楚。`</span></span>
<span><span class="co">#&gt; Text id: 5.a</span></span>
<span><span class="co">#&gt; `那個剛才推了男孩一下的婦人偷了店員并且打傷了她。`</span></span>
<span><span class="co">#&gt; Text id: 6.a</span></span>
<span><span class="co">#&gt; `那個上個月邀請了男孩幾次的女孩認識王老師因為上過她的課。`</span></span>
<span><span class="co">#&gt; Text id: 7.a</span></span>
<span><span class="co">#&gt; `那條去年救了主人好幾次的狗喜歡小男孩所以很興奮。`</span></span>
<span><span class="co">#&gt; Text id: 8.a</span></span>
<span><span class="co">#&gt; `那個剛才推了職業選手一下的業餘選手罵了裁判而且威脅了他。`</span></span>
<span><span class="co">#&gt; Text id: 9.a</span></span>
<span><span class="co">#&gt; `這個上個月監視了殺手一段時間的偵探討厭當地人所以沒有尋求幫助。`</span></span>
<span><span class="co">#&gt; Text id: 10.a</span></span>
<span><span class="co">#&gt; `那位最近抱怨了房東好多次的住戶找了律師而且打算起訴。`</span></span>
<span><span class="co">#&gt; Text id: 11.a</span></span>
<span><span class="co">#&gt; `那個上個月罵了教練一頓的球員愛上了女歌星還送她禮物。`</span></span>
<span><span class="co">#&gt; Text id: 12.a</span></span>
<span><span class="co">#&gt; `那位以前崇拜了指揮家很久的作曲家結識了小提琴手並且兩人常見面。`</span></span>
<span><span class="co">#&gt; Text id: 13.a</span></span>
<span><span class="co">#&gt; `這個去年批評了電影局幾次的女演員很欣賞金城武因為他個性坦率。`</span></span>
<span><span class="co">#&gt; Text id: 14.a</span></span>
<span><span class="co">#&gt; `那位上個月約了飛行員兩次的空姐惹怒了經理因為她常遲到。`</span></span>
<span><span class="co">#&gt; Text id: 15.a</span></span>
<span><span class="co">#&gt; `這位今天稱讚了導演多次的男明星批評了影評家並且表示很難過。`</span></span>
<span><span class="co">#&gt; Text id: 16.a</span></span>
<span><span class="co">#&gt; `那位昨天採訪了作家兩個小時的記者質疑了縣長候選人而且揚言報復。`</span></span>
<span><span class="co">#&gt; Text id: 1.b</span></span>
<span><span class="co">#&gt; `那個昨晚服務生揍了一頓的顧客聽說過老闆並且記得他。`</span></span>
<span><span class="co">#&gt; Text id: 2.b</span></span>
<span><span class="co">#&gt; `那輛下午摩托車追了很久的轎車發現了記者所以停了下來。`</span></span>
<span><span class="co">#&gt; Text id: 3.b</span></span>
<span><span class="co">#&gt; `那個今天男孩打了幾次的女孩看到了校長所以假裝讀書。`</span></span>
<span><span class="co">#&gt; Text id: 4.b</span></span>
<span><span class="co">#&gt; `那輛當時自行車撞了兩次的吉普車攔住了警察並且要求調查清楚。`</span></span>
<span><span class="co">#&gt; Text id: 5.b</span></span>
<span><span class="co">#&gt; `那個剛才男孩推了一下的婦人偷了店員并且打傷了她。`</span></span>
<span><span class="co">#&gt; Text id: 6.b</span></span>
<span><span class="co">#&gt; `那個上個月男孩邀請了幾次的女孩認識王老師因為上過她的課。`</span></span>
<span><span class="co">#&gt; Text id: 7.b</span></span>
<span><span class="co">#&gt; `那條去年主人救了好幾次的狗喜歡小男孩所以很興奮。`</span></span>
<span><span class="co">#&gt; Text id: 8.b</span></span>
<span><span class="co">#&gt; `那個剛才職業選手推了一下的業餘選手罵了裁判而且威脅了他。`</span></span>
<span><span class="co">#&gt; Text id: 9.b</span></span>
<span><span class="co">#&gt; `這個上個月殺手監視了一段時間的偵探討厭當地人所以沒有尋求幫助。`</span></span>
<span><span class="co">#&gt; Text id: 10.b</span></span>
<span><span class="co">#&gt; `那位最近房東抱怨了好多次的住戶找了律師而且打算起訴。`</span></span>
<span><span class="co">#&gt; Text id: 11.b</span></span>
<span><span class="co">#&gt; `那個上個月教練罵了一頓的球員愛上了女歌星還送她禮物。`</span></span>
<span><span class="co">#&gt; Text id: 12.b</span></span>
<span><span class="co">#&gt; `那位以前指揮家崇拜了很久的作曲家結識了小提琴手並且兩人常見面。`</span></span>
<span><span class="co">#&gt; Text id: 13.b</span></span>
<span><span class="co">#&gt; `這個去年電影局批評了幾次的女演員很欣賞金城武因為他個性坦率。`</span></span>
<span><span class="co">#&gt; Text id: 14.b</span></span>
<span><span class="co">#&gt; `那位上個月飛行員約了兩次的空姐惹怒了經理因為她常遲到。`</span></span>
<span><span class="co">#&gt; Text id: 15.b</span></span>
<span><span class="co">#&gt; `這位今天導演稱讚了多次的男明星批評了影評家並且表示很難過。`</span></span>
<span><span class="co">#&gt; Text id: 16.b</span></span>
<span><span class="co">#&gt; `那位昨天作家採訪了兩個小時的記者質疑了縣長候選人而且揚言報復。`</span></span>
<span><span class="co">#&gt; Text id: 1.c</span></span>
<span><span class="co">#&gt; `老闆聽說過那個昨晚揍了服務生一頓的顧客並且通知了記者。`</span></span>
<span><span class="co">#&gt; Text id: 2.c</span></span>
<span><span class="co">#&gt; `記者發現了那輛下午追了摩托車很久的轎車所以停了下來。`</span></span>
<span><span class="co">#&gt; Text id: 3.c</span></span>
<span><span class="co">#&gt; `校長看到了那個今天打了男孩幾次的女孩所以上前詢問。`</span></span>
<span><span class="co">#&gt; Text id: 4.c</span></span>
<span><span class="co">#&gt; `警察攔住了那輛當時撞了自行車兩次的吉普車並且要求調查清楚。`</span></span>
<span><span class="co">#&gt; Text id: 5.c</span></span>
<span><span class="co">#&gt; `店員偷了那個剛才推了男孩一下的婦人并且打傷了她。`</span></span>
<span><span class="co">#&gt; Text id: 6.c</span></span>
<span><span class="co">#&gt; `王老師認識那個上個月邀請了男孩幾次的女孩因為教過她的課。`</span></span>
<span><span class="co">#&gt; Text id: 7.c</span></span>
<span><span class="co">#&gt; `小男孩喜歡那條去年救了主人好幾次的狗所以很興奮。`</span></span>
<span><span class="co">#&gt; Text id: 8.c</span></span>
<span><span class="co">#&gt; `裁判罵了那個剛才推了職業選手一下的業餘選手而且警告了他。`</span></span>
<span><span class="co">#&gt; Text id: 9.c</span></span>
<span><span class="co">#&gt; `當地人討厭這個上個月監視了殺手一段時間的偵探所以沒有提供幫助。`</span></span>
<span><span class="co">#&gt; Text id: 10.c</span></span>
<span><span class="co">#&gt; `律師找了那位最近抱怨了房東好多次的住戶而且打算起訴。`</span></span>
<span><span class="co">#&gt; Text id: 11.c</span></span>
<span><span class="co">#&gt; `女歌星愛上了那個上個月罵了教練一頓的球員還送他禮物。`</span></span>
<span><span class="co">#&gt; Text id: 12.c</span></span>
<span><span class="co">#&gt; `小提琴手結識了那位以前崇拜了指揮家很久的作曲家並且兩人常見面。`</span></span>
<span><span class="co">#&gt; Text id: 13.c</span></span>
<span><span class="co">#&gt; `金城武很欣賞這個去年批評了電影局幾次的女演員因為她個性坦率。`</span></span>
<span><span class="co">#&gt; Text id: 14.c</span></span>
<span><span class="co">#&gt; `經理惹怒了那位上個月約了飛行員兩次的空姐因為他常冷嘲熱諷。`</span></span>
<span><span class="co">#&gt; Text id: 15.c</span></span>
<span><span class="co">#&gt; `影評家批評了這位今天稱讚了導演多次的男明星並且表示很難過。`</span></span>
<span><span class="co">#&gt; Text id: 16.c</span></span>
<span><span class="co">#&gt; `縣長候選人質疑了那位昨天採訪了作家兩個小時的記者而且揚言報復。`</span></span>
<span><span class="co">#&gt; Text id: 1.d</span></span>
<span><span class="co">#&gt; `老闆聽說過那個昨晚服務生揍了一頓的顧客並且通知了記者。`</span></span>
<span><span class="co">#&gt; Text id: 2.d</span></span>
<span><span class="co">#&gt; `記者發現了那輛下午摩托車追了很久的轎車所以停了下來。`</span></span>
<span><span class="co">#&gt; Text id: 3.d</span></span>
<span><span class="co">#&gt; `校長看到了那個今天男孩打了幾次的女孩所以上前詢問。`</span></span>
<span><span class="co">#&gt; Text id: 4.d</span></span>
<span><span class="co">#&gt; `警察攔住了那輛當時自行車撞了兩次的吉普車並且要求調查清楚。`</span></span>
<span><span class="co">#&gt; Text id: 5.d</span></span>
<span><span class="co">#&gt; `店員偷了那個剛才男孩推了一下的婦人并且打傷了她。`</span></span>
<span><span class="co">#&gt; Text id: 6.d</span></span>
<span><span class="co">#&gt; `王老師認識那個上個月男孩邀請了幾次的女孩因為教過她的課。`</span></span>
<span><span class="co">#&gt; Text id: 7.d</span></span>
<span><span class="co">#&gt; `小男孩喜歡那條去年主人救了好幾次的狗所以很興奮。`</span></span>
<span><span class="co">#&gt; Text id: 8.d</span></span>
<span><span class="co">#&gt; `裁判罵了那個剛才職業選手推了一下的業餘選手而且警告了他。`</span></span>
<span><span class="co">#&gt; Text id: 9.d</span></span>
<span><span class="co">#&gt; `當地人討厭這個上個月殺手監視了一段時間的偵探所以沒有提供幫助。`</span></span>
<span><span class="co">#&gt; Text id: 10.d</span></span>
<span><span class="co">#&gt; `律師找了那位最近房東抱怨了好多次的住戶而且打算起訴。`</span></span>
<span><span class="co">#&gt; Text id: 11.d</span></span>
<span><span class="co">#&gt; `女歌星愛上了那個上個月教練罵了一頓的球員還送他禮物。`</span></span>
<span><span class="co">#&gt; Text id: 12.d</span></span>
<span><span class="co">#&gt; `小提琴手結識了那位以前指揮家崇拜了很久的作曲家並且兩人常見面。`</span></span>
<span><span class="co">#&gt; Text id: 13.d</span></span>
<span><span class="co">#&gt; `金城武很欣賞這個去年電影局批評了幾次的女演員因為她個性坦率。`</span></span>
<span><span class="co">#&gt; Text id: 14.d</span></span>
<span><span class="co">#&gt; `經理惹怒了那位上個月飛行員約了兩次的空姐因為他常冷嘲熱諷。`</span></span>
<span><span class="co">#&gt; Text id: 15.d</span></span>
<span><span class="co">#&gt; `影評家批評了這位今天導演稱讚了多次的男明星並且表示很難過。`</span></span>
<span><span class="co">#&gt; Text id: 16.d</span></span>
<span><span class="co">#&gt; `縣長候選人質疑了那位昨天作家採訪了兩個小時的記者而且揚言報復。`</span></span>
<span><span class="co">#&gt; ***</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html" class="external-link">toc</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; 20.791 sec elapsed</span></span></code></pre></div>
<p>Inspect the calculated surprisal values:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df_stim</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tidytable: 704 × 5</span></span></span>
<span><span class="co">#&gt;     item cond  word       wordn surprisal</span></span>
<span><span class="co">#&gt;    <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;int&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 1</span>     1 a     那個           1     21.8 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 2</span>     1 a     昨晚           2     19.8 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 3</span>     1 a     揍了服務生     3     36.5 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 4</span>     1 a     一頓           4      4.94</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 5</span>     1 a     的             5      2.38</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 6</span>     1 a     顧客           6     11.0 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 7</span>     1 a     聽說過         7     17.6 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 8</span>     1 a     老闆           8      8.84</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;"> 9</span>     1 a     並且           9     12.8 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">10</span>     1 a     記得          10      8.51</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">11</span>     1 a     他。          11     16.5 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">12</span>     2 b     那輛           1     34.4 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">13</span>     2 b     下午           2     15.2 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">14</span>     2 b     摩托車追了     3     34.3 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">15</span>     2 b     很久           4      5.64</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">16</span>     2 b     的             5      5.21</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">17</span>     2 b     轎車           6     15.4 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">18</span>     2 b     發現了         7     13.8 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">19</span>     2 b     記者           8     13.4 </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">20</span>     2 b     所以           9      9.72</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># ℹ 684 more rows</span></span></span></code></pre></div>
<p>The surprisal values are merged back into the original dataset:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df_jaeger14</span> <span class="op">&lt;-</span> <span class="va">df_jaeger14</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://markfairbanks.github.io/tidytable/reference/left_join.html" class="external-link">left_join</a></span><span class="op">(</span><span class="va">df_stim</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="analyze-the-dataset-with-a-bayesian-hierarchical-model">2. Analyze the dataset with a Bayesian hierarchical model<a class="anchor" aria-label="anchor" href="#analyze-the-dataset-with-a-bayesian-hierarchical-model"></a>
</h3>
<p>We estimate the effect of surprisal on reading times (RTs) using a
Bayesian hierarchical model. The model incorporates a hierarchical
structure with by-subject and by-item group-level effects (also referred
to as random effects), which account for variability across subjects and
items. The only population-level effect (also called a fixed effect) of
interest in this analysis is surprisal. (For simplicity, we do not scale
or center the predictor.)</p>
<p>For a detailed discussion of Bayesian hierarchical structures
(including prior selection), see Section 4.2 and Chapter 5 of <span class="citation">Nicenboim, Schad, and Vasishth (2025)</span>, <a href="https://bruno.nicenboim.me/bayescogsci/" class="external-link"><em>An Introduction to
Bayesian Data Analysis for Cognitive Science</em></a>.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_surp</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://paulbuerkner.com/brms/reference/brm.html" class="external-link">brm</a></span><span class="op">(</span><span class="va">rt</span> <span class="op">~</span> <span class="va">surprisal</span> <span class="op">+</span> <span class="op">(</span><span class="va">surprisal</span><span class="op">|</span> <span class="va">subject</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="va">surprisal</span><span class="op">|</span> <span class="va">item</span><span class="op">)</span>,</span>
<span>      data <span class="op">=</span> <span class="va">df_jaeger14</span>,</span>
<span>      family <span class="op">=</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/brmsfamily.html" class="external-link">lognormal</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>      prior <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://paulbuerkner.com/brms/reference/set_prior.html" class="external-link">prior</a></span><span class="op">(</span><span class="fu">normal</span><span class="op">(</span><span class="fl">6</span>, <span class="fl">1.5</span><span class="op">)</span>, class <span class="op">=</span> <span class="va">Intercept</span><span class="op">)</span>,</span>
<span>                <span class="fu"><a href="https://paulbuerkner.com/brms/reference/set_prior.html" class="external-link">prior</a></span><span class="op">(</span><span class="fu">normal</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, class <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span>,</span>
<span>                <span class="fu"><a href="https://paulbuerkner.com/brms/reference/set_prior.html" class="external-link">prior</a></span><span class="op">(</span><span class="fu">normal</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, class <span class="op">=</span> <span class="va">sd</span><span class="op">)</span>,</span>
<span>                <span class="fu"><a href="https://paulbuerkner.com/brms/reference/set_prior.html" class="external-link">prior</a></span><span class="op">(</span><span class="fu">normal</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">.1</span><span class="op">)</span>, class <span class="op">=</span> <span class="va">b</span><span class="op">)</span>,</span>
<span>                <span class="fu"><a href="https://paulbuerkner.com/brms/reference/set_prior.html" class="external-link">prior</a></span><span class="op">(</span><span class="fu">lkj</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>, class <span class="op">=</span> <span class="va">cor</span><span class="op">)</span><span class="op">)</span>,</span>
<span>      iter <span class="op">=</span> <span class="fl">3000</span><span class="op">)</span></span></code></pre></div>
<p>We visualize the posterior distributions and model diagnostics:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_surp</span><span class="op">)</span></span></code></pre></div>
<p><img src="example_files/figure-html/unnamed-chunk-8-1.png" width="700"><img src="example_files/figure-html/unnamed-chunk-8-2.png" width="700"></p>
<p>The effect of surprisal (in bits) on log-RT is summarized below:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://paulbuerkner.com/brms/reference/posterior_summary.html" class="external-link">posterior_summary</a></span><span class="op">(</span><span class="va">fit_surp</span>, variable <span class="op">=</span> <span class="st">"b_surprisal"</span><span class="op">)</span></span>
<span><span class="co">#&gt;                Estimate   Est.Error        Q2.5      Q97.5</span></span>
<span><span class="co">#&gt; b_surprisal 0.007105797 0.001171488 0.004726614 0.00939844</span></span></code></pre></div>
<p>The conditional effects of surprisal on the original millisecond
scale can be plotted as well:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://paulbuerkner.com/brms/reference/conditional_effects.brmsfit.html" class="external-link">conditional_effects</a></span><span class="op">(</span><span class="va">fit_surp</span><span class="op">)</span></span></code></pre></div>
<p><img src="example_files/figure-html/unnamed-chunk-10-1.png" width="700"></p>
</div>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>This vignette demonstrates how to calculate surprisal values using
the <code>pangoling</code> package and estimate their effect on reading
times using Bayesian hierarchical modeling. By leveraging pre-trained
transformer models, researchers can explore the cognitive effects of
surprisal in language comprehension experiments.</p>
<p><span class="citation">Levy (2008)</span> theory highlights how
surprisal acts as a cognitive bottleneck: higher surprisal reflects
greater unexpectedness, which slows down processing due to the
reallocation of cognitive resources. This example illustrates how models
like GPT-2 can operationalize surprisal and facilitate testing such
theories in empirical data.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-hale2001probabilistic" class="csl-entry">
Hale, John. 2001. <span>“A Probabilistic Earley Parser as a
Psycholinguistic Model.”</span> In <em>Second Meeting of the North
American Chapter of the Association for Computational Linguistics</em>.
</div>
<div id="ref-JaegerEtAl2015" class="csl-entry">
Jäger, Lena, Zhong Chen, Qiang Li, Chien-Jer Charles Lin, and Shravan
Vasishth. 2015. <span>“The Subject-Relative Advantage in Chinese:
Evidence for Expectation-Based Processing.”</span> <em>Journal of Memory
and Language</em> 79-80: 97–120. https://doi.org/<a href="https://doi.org/10.1016/j.jml.2014.10.005" class="external-link">https://doi.org/10.1016/j.jml.2014.10.005</a>.
</div>
<div id="ref-levy2008" class="csl-entry">
Levy, Roger. 2008. <span>“Expectation-Based Syntactic
Comprehension.”</span> <em>Cognition</em> 106 (3): 1126–77. <a href="https://doi.org/10.1016/j.cognition.2007.05.006" class="external-link">https://doi.org/10.1016/j.cognition.2007.05.006</a>.
</div>
<div id="ref-nicenboim2025introduction" class="csl-entry">
Nicenboim, Bruno, Daniel Schad, and Shravan Vasishth. 2025. <em>An
Introduction to Bayesian Data Analysis for Cognitive Science</em>.
<em>Under Contract with Chapman and Hall/CRC Statistics in the Social
and Behavioral Sciences Series</em>. <a href="https://bruno.nicenboim.me/bayescogsci/" class="external-link">https://bruno.nicenboim.me/bayescogsci/</a>.
</div>
<div id="ref-shannon1948mathematical" class="csl-entry">
Shannon, Claude Elwood. 1948. <span>“A Mathematical Theory of
Communication.”</span> <em>The Bell System Technical Journal</em> 27
(3): 379–423.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://bruno.nicenboim.me/" class="external-link">Bruno Nicenboim</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  
</body>
</html>
