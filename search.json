[{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to pangoling","title":"Contributing to pangoling","text":"outlines propose change pangoling. detailed discussion contributing tidyverse packages, please see development contributing guide code review principles.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to pangoling","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to pangoling","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). See guide create great issue advice.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to pangoling","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"bnicenboim/pangoling\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to pangoling","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of conduct","title":"Contributing to pangoling","text":"Please note package released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 Bruno Nicenboim Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/articles/example.html","id":"preprocessing","dir":"Articles","previous_headings":"Data analysis","what":"0. Preprocessing","title":"Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading","text":"load required libraries first. dataset df_jaeger14 Jäger et al. (2015) contains Chinese sentences presented word--word participants. word associated reaction time (RT). (original research question experimental conditions don’t matter us.) avoid redundant computations, extract unique combinations items, conditions, words. (don’t want run functions entire dataset repetition across subjects, thus model run several times identical words identical sentences): use pangoling function causal_preload() load GPT-2 model uer/gpt2-chinese-cluecorpussmall. model trained Chinese texts can predict word probabilities based preceding context.","code":"library(pangoling) library(tidytable) # Lightweight and faster alternative to dplyr library(tictoc)  # Report the time that a piece of code takes library(brms) # Bayesian regression models in Stan # Use all the cores needed available for brms: options(mc.cores = parallel::detectCores()) data(\"df_jaeger14\") df_jaeger14 #> # A tidytable: 8,624 × 14 #>    subject  item cond  word  wordn    rt region question accuracy correct_answer #>    <chr>   <int> <chr> <chr> <int> <int> <fct>  <chr>       <int>          <int> #>  1 1m1         1 a     那個      1   360 Det+CL 那個顧客聽說過…        1              1 #>  2 1m1         1 a     昨晚      2   359 Adv    那個顧客聽說過…        1              1 #>  3 1m1         1 a     揍了服務…     3   344 VN     那個顧客聽說過…        1              1 #>  4 1m1         1 a     一頓      4   313 FreqP  那個顧客聽說過…        1              1 #>  5 1m1         1 a     的        5   297 DE     那個顧客聽說過…        1              1 #>  6 1m1         1 a     顧客      6   312 head   那個顧客聽說過…        1              1 #>  7 1m1         1 a     聽說過……     7   297 hd1    那個顧客聽說過…        1              1 #>  8 1m1         1 a     老闆      8   281 hd2    那個顧客聽說過…        1              1 #>  9 1m1         1 a     並且      9   297 hd3    那個顧客聽說過…        1              1 #> 10 1m1         1 a     記得     10   313 hd4    那個顧客聽說過…        1              1 #> # ℹ 8,614 more rows #> # ℹ 4 more variables: question_type <int>, experiment <chr>, list <int>, #> #   sentence <chr> df_stim <- df_jaeger14 |> distinct(item, cond, word, wordn) df_stim |> print(n = 20) #> # A tidytable: 704 × 4 #>     item cond  word       wordn #>    <int> <chr> <chr>      <int> #>  1     1 a     那個           1 #>  2     1 a     昨晚           2 #>  3     1 a     揍了服務生     3 #>  4     1 a     一頓           4 #>  5     1 a     的             5 #>  6     1 a     顧客           6 #>  7     1 a     聽說過         7 #>  8     1 a     老闆           8 #>  9     1 a     並且           9 #> 10     1 a     記得          10 #> 11     1 a     他。          11 #> 12     2 b     那輛           1 #> 13     2 b     下午           2 #> 14     2 b     摩托車追了     3 #> 15     2 b     很久           4 #> 16     2 b     的             5 #> 17     2 b     轎車           6 #> 18     2 b     發現了         7 #> 19     2 b     記者           8 #> 20     2 b     所以           9 #> # ℹ 684 more rows causal_preload(\"uer/gpt2-chinese-cluecorpussmall\") #> Preloading causal model uer/gpt2-chinese-cluecorpussmall..."},{"path":"http://bruno.nicenboim.me/pangoling/articles/example.html","id":"add-surprisal-values-to-the-dataset","dir":"Articles","previous_headings":"Data analysis","what":"1. Add surprisal values to the dataset","title":"Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading","text":"Surprisal bits (.e., use logarithm base 2) calculated −log2(P(word|context))-\\log_2(P(\\text{word}|\\text{context})), equivalent log1/2(P(word|context))\\log_{1/2}(P(\\text{word}|\\text{context})). pangoling function causal_words_pred() configured : Use log.p = 0.5 get surprisal bits. Set sep = \"\" since Chinese words separated spaces. Process sentences batches 10 efficiency. Inspect calculated surprisal values: surprisal values merged back original dataset:","code":"tic() df_stim <- df_stim |>   mutate(surprisal = causal_words_pred(     word,     by = interaction(item, cond),     log.p = 0.5,     model = \"uer/gpt2-chinese-cluecorpussmall\",     sep = \"\",     batch_size = 10   )) #> Processing using causal model 'uer/gpt2-chinese-cluecorpussmall/' ... #> Processing a batch of size 10 with 33 tokens. #> Processing a batch of size 10 with 33 tokens. #> Processing a batch of size 10 with 33 tokens. #> Processing a batch of size 10 with 33 tokens. #> Processing a batch of size 10 with 33 tokens. #> Processing a batch of size 10 with 33 tokens. #> Processing a batch of size 4 with 33 tokens. #> Text id: 1.a #> `那個昨晚揍了服務生一頓的顧客聽說過老闆並且記得他。` #> Text id: 2.a #> `那輛下午追了摩托車很久的轎車發現了記者所以停了下來。` #> Text id: 3.a #> `那個今天打了男孩幾次的女孩看到了校長所以假裝讀書。` #> Text id: 4.a #> `那輛當時撞了自行車兩次的吉普車攔住了警察並且要求調查清楚。` #> Text id: 5.a #> `那個剛才推了男孩一下的婦人偷了店員并且打傷了她。` #> Text id: 6.a #> `那個上個月邀請了男孩幾次的女孩認識王老師因為上過她的課。` #> Text id: 7.a #> `那條去年救了主人好幾次的狗喜歡小男孩所以很興奮。` #> Text id: 8.a #> `那個剛才推了職業選手一下的業餘選手罵了裁判而且威脅了他。` #> Text id: 9.a #> `這個上個月監視了殺手一段時間的偵探討厭當地人所以沒有尋求幫助。` #> Text id: 10.a #> `那位最近抱怨了房東好多次的住戶找了律師而且打算起訴。` #> Text id: 11.a #> `那個上個月罵了教練一頓的球員愛上了女歌星還送她禮物。` #> Text id: 12.a #> `那位以前崇拜了指揮家很久的作曲家結識了小提琴手並且兩人常見面。` #> Text id: 13.a #> `這個去年批評了電影局幾次的女演員很欣賞金城武因為他個性坦率。` #> Text id: 14.a #> `那位上個月約了飛行員兩次的空姐惹怒了經理因為她常遲到。` #> Text id: 15.a #> `這位今天稱讚了導演多次的男明星批評了影評家並且表示很難過。` #> Text id: 16.a #> `那位昨天採訪了作家兩個小時的記者質疑了縣長候選人而且揚言報復。` #> Text id: 1.b #> `那個昨晚服務生揍了一頓的顧客聽說過老闆並且記得他。` #> Text id: 2.b #> `那輛下午摩托車追了很久的轎車發現了記者所以停了下來。` #> Text id: 3.b #> `那個今天男孩打了幾次的女孩看到了校長所以假裝讀書。` #> Text id: 4.b #> `那輛當時自行車撞了兩次的吉普車攔住了警察並且要求調查清楚。` #> Text id: 5.b #> `那個剛才男孩推了一下的婦人偷了店員并且打傷了她。` #> Text id: 6.b #> `那個上個月男孩邀請了幾次的女孩認識王老師因為上過她的課。` #> Text id: 7.b #> `那條去年主人救了好幾次的狗喜歡小男孩所以很興奮。` #> Text id: 8.b #> `那個剛才職業選手推了一下的業餘選手罵了裁判而且威脅了他。` #> Text id: 9.b #> `這個上個月殺手監視了一段時間的偵探討厭當地人所以沒有尋求幫助。` #> Text id: 10.b #> `那位最近房東抱怨了好多次的住戶找了律師而且打算起訴。` #> Text id: 11.b #> `那個上個月教練罵了一頓的球員愛上了女歌星還送她禮物。` #> Text id: 12.b #> `那位以前指揮家崇拜了很久的作曲家結識了小提琴手並且兩人常見面。` #> Text id: 13.b #> `這個去年電影局批評了幾次的女演員很欣賞金城武因為他個性坦率。` #> Text id: 14.b #> `那位上個月飛行員約了兩次的空姐惹怒了經理因為她常遲到。` #> Text id: 15.b #> `這位今天導演稱讚了多次的男明星批評了影評家並且表示很難過。` #> Text id: 16.b #> `那位昨天作家採訪了兩個小時的記者質疑了縣長候選人而且揚言報復。` #> Text id: 1.c #> `老闆聽說過那個昨晚揍了服務生一頓的顧客並且通知了記者。` #> Text id: 2.c #> `記者發現了那輛下午追了摩托車很久的轎車所以停了下來。` #> Text id: 3.c #> `校長看到了那個今天打了男孩幾次的女孩所以上前詢問。` #> Text id: 4.c #> `警察攔住了那輛當時撞了自行車兩次的吉普車並且要求調查清楚。` #> Text id: 5.c #> `店員偷了那個剛才推了男孩一下的婦人并且打傷了她。` #> Text id: 6.c #> `王老師認識那個上個月邀請了男孩幾次的女孩因為教過她的課。` #> Text id: 7.c #> `小男孩喜歡那條去年救了主人好幾次的狗所以很興奮。` #> Text id: 8.c #> `裁判罵了那個剛才推了職業選手一下的業餘選手而且警告了他。` #> Text id: 9.c #> `當地人討厭這個上個月監視了殺手一段時間的偵探所以沒有提供幫助。` #> Text id: 10.c #> `律師找了那位最近抱怨了房東好多次的住戶而且打算起訴。` #> Text id: 11.c #> `女歌星愛上了那個上個月罵了教練一頓的球員還送他禮物。` #> Text id: 12.c #> `小提琴手結識了那位以前崇拜了指揮家很久的作曲家並且兩人常見面。` #> Text id: 13.c #> `金城武很欣賞這個去年批評了電影局幾次的女演員因為她個性坦率。` #> Text id: 14.c #> `經理惹怒了那位上個月約了飛行員兩次的空姐因為他常冷嘲熱諷。` #> Text id: 15.c #> `影評家批評了這位今天稱讚了導演多次的男明星並且表示很難過。` #> Text id: 16.c #> `縣長候選人質疑了那位昨天採訪了作家兩個小時的記者而且揚言報復。` #> Text id: 1.d #> `老闆聽說過那個昨晚服務生揍了一頓的顧客並且通知了記者。` #> Text id: 2.d #> `記者發現了那輛下午摩托車追了很久的轎車所以停了下來。` #> Text id: 3.d #> `校長看到了那個今天男孩打了幾次的女孩所以上前詢問。` #> Text id: 4.d #> `警察攔住了那輛當時自行車撞了兩次的吉普車並且要求調查清楚。` #> Text id: 5.d #> `店員偷了那個剛才男孩推了一下的婦人并且打傷了她。` #> Text id: 6.d #> `王老師認識那個上個月男孩邀請了幾次的女孩因為教過她的課。` #> Text id: 7.d #> `小男孩喜歡那條去年主人救了好幾次的狗所以很興奮。` #> Text id: 8.d #> `裁判罵了那個剛才職業選手推了一下的業餘選手而且警告了他。` #> Text id: 9.d #> `當地人討厭這個上個月殺手監視了一段時間的偵探所以沒有提供幫助。` #> Text id: 10.d #> `律師找了那位最近房東抱怨了好多次的住戶而且打算起訴。` #> Text id: 11.d #> `女歌星愛上了那個上個月教練罵了一頓的球員還送他禮物。` #> Text id: 12.d #> `小提琴手結識了那位以前指揮家崇拜了很久的作曲家並且兩人常見面。` #> Text id: 13.d #> `金城武很欣賞這個去年電影局批評了幾次的女演員因為她個性坦率。` #> Text id: 14.d #> `經理惹怒了那位上個月飛行員約了兩次的空姐因為他常冷嘲熱諷。` #> Text id: 15.d #> `影評家批評了這位今天導演稱讚了多次的男明星並且表示很難過。` #> Text id: 16.d #> `縣長候選人質疑了那位昨天作家採訪了兩個小時的記者而且揚言報復。` #> *** toc() #> 22.741 sec elapsed df_stim |> print(n = 20) #> # A tidytable: 704 × 5 #>     item cond  word       wordn surprisal #>    <int> <chr> <chr>      <int>     <dbl> #>  1     1 a     那個           1     21.8  #>  2     1 a     昨晚           2     19.8  #>  3     1 a     揍了服務生     3     36.5  #>  4     1 a     一頓           4      4.94 #>  5     1 a     的             5      2.38 #>  6     1 a     顧客           6     11.0  #>  7     1 a     聽說過         7     17.6  #>  8     1 a     老闆           8      8.84 #>  9     1 a     並且           9     12.8  #> 10     1 a     記得          10      8.51 #> 11     1 a     他。          11     16.5  #> 12     2 b     那輛           1     34.4  #> 13     2 b     下午           2     15.2  #> 14     2 b     摩托車追了     3     34.3  #> 15     2 b     很久           4      5.64 #> 16     2 b     的             5      5.21 #> 17     2 b     轎車           6     15.4  #> 18     2 b     發現了         7     13.8  #> 19     2 b     記者           8     13.4  #> 20     2 b     所以           9      9.72 #> # ℹ 684 more rows df_jaeger14 <- df_jaeger14 |> left_join(df_stim)"},{"path":"http://bruno.nicenboim.me/pangoling/articles/example.html","id":"analyze-the-dataset-with-a-bayesian-hierarchical-model","dir":"Articles","previous_headings":"Data analysis","what":"2. Analyze the dataset with a Bayesian hierarchical model","title":"Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading","text":"estimate effect surprisal reading times (RTs) using Bayesian hierarchical model. model incorporates hierarchical structure -subject -item group-level effects (also referred random effects), account variability across subjects items. population-level effect (also called fixed effect) interest analysis surprisal. (simplicity, scale center predictor.) detailed discussion Bayesian hierarchical structures (including prior selection), see Section 4.2 Chapter 5 Nicenboim, Schad, Vasishth (2025), Introduction Bayesian Data Analysis Cognitive Science. visualize posterior distributions model diagnostics:  effect surprisal (bits) log-RT summarized : conditional effects surprisal original millisecond scale can plotted well:","code":"fit_surp <-   brm(rt ~ surprisal + (surprisal| subject) + (surprisal| item),       data = df_jaeger14,       family = lognormal(),       prior = c(prior(normal(6, 1.5), class = Intercept),                 prior(normal(0, 1), class = sigma),                 prior(normal(0, 1), class = sd),                 prior(normal(0, .1), class = b),                 prior(lkj(2), class = cor)),       iter = 3000) plot(fit_surp) posterior_summary(fit_surp, variable = \"b_surprisal\") #>                Estimate   Est.Error        Q2.5       Q97.5 #> b_surprisal 0.007087394 0.001157347 0.004751983 0.009353576 conditional_effects(fit_surp)"},{"path":"http://bruno.nicenboim.me/pangoling/articles/example.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Worked-out example: Surprisal from a causal (GPT) model as a cognitive processing bottleneck in reading","text":"vignette demonstrates calculate surprisal values using pangoling package estimate effect reading times using Bayesian hierarchical modeling. leveraging pre-trained transformer models, researchers can explore cognitive effects surprisal language comprehension experiments. Levy (2008) theory highlights surprisal acts cognitive bottleneck: higher surprisal reflects greater unexpectedness, slows processing due reallocation cognitive resources. example illustrates models like GPT-2 can operationalize surprisal facilitate testing theories empirical data.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/articles/troubleshooting.html","id":"module-not-found-error-in-rstudio","dir":"Articles","previous_headings":"","what":"Module not found error in Rstudio","title":"Troubleshooting the use of Python in R","text":"might encounter error message similar : might mean python dependencies installed. pangoling provides convienence function install necessary python dependencies conda environment: pangoling (many R packages) relies reticulate Python functionality. Even package installation seems successful, issue may reticulate correctly loading correct Python environment. default, R use conda environment named r-reticulate managing configurations automatically. One can verify using py_config, output linux computer conda environment wasn’t loaded: One can anyways configure RStudio load correct conda environment (r-reticulate) default following steps: RStudio, navigate “Tools” menu. Click “Global Options” “Tools” menu. Click Python.  Click Select… Click Conda Enviornment tab. Click r-reticulate path.  Click Select button bottom. path selected appear now using py_config():","code":"Error in py_run_string_impl(code, local, convert) :    ModuleNotFoundError: No module named 'torch' Run `reticulate::py_last_error()` for details. install_py_pangoling() library(reticulate) py_config() #> python:         /usr/local/bin/python #> libpython:      /usr/lib/python3.10/config-3.10-x86_64-linux-gnu/libpython3.10.so #> pythonhome:     //usr://usr #> version:        3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] #> numpy:           [NOT FOUND] #>  #> NOTE: Python version was forced by RETICULATE_PYTHON_FALLBACK py_config() #> python:         /home/runner/.virtualenvs/r-reticulate/bin/python #> libpython:      /opt/hostedtoolcache/Python/3.9.21/x64/lib/libpython3.9.so #> pythonhome:     /home/runner/.virtualenvs/r-reticulate:/home/runner/.virtualenvs/r-reticulate #> version:        3.9.21 (main, Dec 12 2024, 19:08:08)  [GCC 13.2.0] #> numpy:          /home/runner/.virtualenvs/r-reticulate/lib/python3.9/site-packages/numpy #> numpy_version:  2.0.2 #>  #> NOTE: Python version was forced by RETICULATE_PYTHON"},{"path":"http://bruno.nicenboim.me/pangoling/articles/troubleshooting.html","id":"httpsconnectionpool-error","dir":"Articles","previous_headings":"","what":"HTTPSConnectionPool error","title":"Troubleshooting the use of Python in R","text":"causal_ masked_ commands throws error starts follows: first time model run, download files available subsequent runs. internet connection (huggingface website ) first run, one experience problem. Afterwards, possible use pangoling without internet connection.","code":"Error in py_run_string_impl(code, local, convert) : requests.exceptions.SSLError: HTTPSConnectionPool(host='huggingface.co', port=443):"},{"path":"http://bruno.nicenboim.me/pangoling/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bruno Nicenboim. Author, maintainer. Chris Emmerly. Contributor. Giovanni Cassani. Contributor. Lisa Levinson. Reviewer. Utku Turk. Reviewer.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Nicenboim B (2023). pangoling: Access large language model predictions R. doi:10.5281/zenodo.7637526, R package version 0.0.0.9011, https://github.com/bnicenboim/pangoling.","code":"@Manual{,   title = {{pangoling}: {Access} to large language model predictions in {R}},   author = {Bruno Nicenboim},   year = {2023},   note = {R package version 0.0.0.9011},   doi = {10.5281/zenodo.7637526},   url = {https://github.com/bnicenboim/pangoling}, }"},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"pangoling-","dir":"","previous_headings":"","what":"Access to Large Language Model Predictions in R","title":"Access to Large Language Model Predictions in R","text":"pangoling1 R package estimating predictability words given context using transformer models. package provides interface utilizing pre-trained transformer models (GPT-2 BERT) obtain word probabilities. word probabilities often utilized predictors psycholinguistic studies. package can useful researchers field psycholinguistics want leverage power transformer models work. package mostly wrapper python package transformers process data convenient format.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"important-limitations-and-bias","dir":"","previous_headings":"","what":"Important! Limitations and bias","title":"Access to Large Language Model Predictions in R","text":"training data popular models (GPT-2) haven’t released, one inspect . ’s clear data contain lot unfiltered content internet, far neutral. See example scope openAI team’s model card GPT-2, many models, limitations bias section GPT-2 Hugging Face website.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Access to Large Language Model Predictions in R","text":"still released version pangoling. package ** early** stages development, probably subject changes. install latest version github use: install_py_pangoling function facilitates installation Python packages needed using pangoling within R environment, using reticulate package managing Python environments. needs done .","code":"# install.packages(\"remotes\") # if needed remotes::install_github(\"bnicenboim/pangoling\") install_py_pangoling()"},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Access to Large Language Model Predictions in R","text":"basic example shows get log-probabilities words dataset: Given (toy) dataset sentences organized one word short phrase row: One can get log-transformed probability word based GPT-2 follows:","code":"library(pangoling) library(tidytable) #fast alternative to dplyr sentences <- c(\"The apple doesn't fall far from the tree.\",                 \"Don't judge a book by its cover.\") (df_sent <- strsplit(x = sentences, split = \" \") |>    map_dfr(.f =  ~ data.frame(word = .x), .id = \"sent_n\")) #> # A tidytable: 15 × 2 #>    sent_n word    #>     <int> <chr>   #>  1      1 The     #>  2      1 apple   #>  3      1 doesn't #>  4      1 fall    #>  5      1 far     #>  6      1 from    #>  7      1 the     #>  8      1 tree.   #>  9      2 Don't   #> 10      2 judge   #> 11      2 a       #> 12      2 book    #> 13      2 by      #> 14      2 its     #> 15      2 cover. df_sent <- df_sent |>   mutate(lp = causal_words_pred(word, by = sent_n)) #> Processing using causal model 'gpt2/' ... #> Processing a batch of size 1 with 10 tokens. #> Processing a batch of size 1 with 9 tokens. #> Text id: 1 #> `The apple doesn't fall far from the tree.` #> Text id: 2 #> `Don't judge a book by its cover.` df_sent #> # A tidytable: 15 × 3 #>    sent_n word         lp #>     <int> <chr>     <dbl> #>  1      1 The      NA     #>  2      1 apple   -10.9   #>  3      1 doesn't  -5.50  #>  4      1 fall     -3.60  #>  5      1 far      -2.91  #>  6      1 from     -0.745 #>  7      1 the      -0.207 #>  8      1 tree.    -1.58  #>  9      2 Don't    NA     #> 10      2 judge    -6.27  #> 11      2 a        -2.33  #> 12      2 book     -1.97  #> 13      2 by       -0.409 #> 14      2 its      -0.257 #> 15      2 cover.   -1.38"},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"how-to-cite","dir":"","previous_headings":"","what":"How to cite","title":"Access to Large Language Model Predictions in R","text":"Nicenboim B (2023). pangoling: Access language model predictions R. R package version 0.0.0.9010, DOI: 10.5281/zenodo.7637526, https://github.com/bnicenboim/pangoling.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"how-to-contribute","dir":"","previous_headings":"","what":"How to contribute","title":"Access to Large Language Model Predictions in R","text":"See Contributing guidelines.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of conduct","title":"Access to Large Language Model Predictions in R","text":"Please note package released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"see-also","dir":"","previous_headings":"","what":"See also","title":"Access to Large Language Model Predictions in R","text":"Another R package act wrapper transformers text However, text general, focus Natural Language Processing Machine Learning.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the configuration of a causal model — causal_config","title":"Returns the configuration of a causal model — causal_config","text":"Returns configuration causal model","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the configuration of a causal model — causal_config","text":"","code":"causal_config(   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   config_model = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the configuration of a causal model — causal_config","text":"model Name pre-trained model folder. One able use models based \"gpt2\". See hugging face website. checkpoint Folder checkpoint. config_model List arguments control model Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the configuration of a causal model — causal_config","text":"list configuration model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"more-details-about-causal-models","dir":"Reference","previous_headings":"","what":"More details about causal models","title":"Returns the configuration of a causal model — causal_config","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the configuration of a causal model — causal_config","text":"","code":"causal_config(model = \"gpt2\") #> $vocab_size #> [1] 50257 #>  #> $n_positions #> [1] 1024 #>  #> $n_embd #> [1] 768 #>  #> $n_layer #> [1] 12 #>  #> $n_head #> [1] 12 #>  #> $n_inner #> NULL #>  #> $activation_function #> [1] \"gelu_new\" #>  #> $resid_pdrop #> [1] 0.1 #>  #> $embd_pdrop #> [1] 0.1 #>  #> $attn_pdrop #> [1] 0.1 #>  #> $layer_norm_epsilon #> [1] 1e-05 #>  #> $initializer_range #> [1] 0.02 #>  #> $summary_type #> [1] \"cls_index\" #>  #> $summary_use_proj #> [1] TRUE #>  #> $summary_activation #> NULL #>  #> $summary_first_dropout #> [1] 0.1 #>  #> $summary_proj_to_labels #> [1] TRUE #>  #> $scale_attn_weights #> [1] TRUE #>  #> $use_cache #> [1] TRUE #>  #> $scale_attn_by_inverse_layer_idx #> [1] FALSE #>  #> $reorder_and_upcast_attn #> [1] FALSE #>  #> $bos_token_id #> [1] 50256 #>  #> $eos_token_id #> [1] 50256 #>  #> $return_dict #> [1] TRUE #>  #> $output_hidden_states #> [1] FALSE #>  #> $output_attentions #> [1] FALSE #>  #> $torchscript #> [1] FALSE #>  #> $torch_dtype #> [1] \"float32\" #>  #> $use_bfloat16 #> [1] FALSE #>  #> $tf_legacy_loss #> [1] FALSE #>  #> $pruned_heads #> named list() #>  #> $tie_word_embeddings #> [1] TRUE #>  #> $chunk_size_feed_forward #> [1] 0 #>  #> $is_encoder_decoder #> [1] FALSE #>  #> $is_decoder #> [1] FALSE #>  #> $cross_attention_hidden_size #> NULL #>  #> $add_cross_attention #> [1] FALSE #>  #> $tie_encoder_decoder #> [1] FALSE #>  #> $max_length #> [1] 20 #>  #> $min_length #> [1] 0 #>  #> $do_sample #> [1] FALSE #>  #> $early_stopping #> [1] FALSE #>  #> $num_beams #> [1] 1 #>  #> $num_beam_groups #> [1] 1 #>  #> $diversity_penalty #> [1] 0 #>  #> $temperature #> [1] 1 #>  #> $top_k #> [1] 50 #>  #> $top_p #> [1] 1 #>  #> $typical_p #> [1] 1 #>  #> $repetition_penalty #> [1] 1 #>  #> $length_penalty #> [1] 1 #>  #> $no_repeat_ngram_size #> [1] 0 #>  #> $encoder_no_repeat_ngram_size #> [1] 0 #>  #> $bad_words_ids #> NULL #>  #> $num_return_sequences #> [1] 1 #>  #> $output_scores #> [1] FALSE #>  #> $return_dict_in_generate #> [1] \"TRUE\" #>  #> $forced_bos_token_id #> NULL #>  #> $forced_eos_token_id #> NULL #>  #> $remove_invalid_values #> [1] FALSE #>  #> $exponential_decay_length_penalty #> NULL #>  #> $suppress_tokens #> NULL #>  #> $begin_suppress_tokens #> NULL #>  #> $architectures #> [1] \"GPT2LMHeadModel\" #>  #> $finetuning_task #> NULL #>  #> $id2label #> $id2label$`0` #> [1] \"LABEL_0\" #>  #> $id2label$`1` #> [1] \"LABEL_1\" #>  #>  #> $label2id #> $label2id$LABEL_0 #> [1] 0 #>  #> $label2id$LABEL_1 #> [1] 1 #>  #>  #> $tokenizer_class #> NULL #>  #> $prefix #> NULL #>  #> $pad_token_id #> NULL #>  #> $sep_token_id #> NULL #>  #> $decoder_start_token_id #> NULL #>  #> $task_specific_params #> $task_specific_params$`text-generation` #> $task_specific_params$`text-generation`$do_sample #> [1] TRUE #>  #> $task_specific_params$`text-generation`$max_length #> [1] 50 #>  #>  #>  #> $problem_type #> NULL #>  #> $`_name_or_path` #> [1] \"gpt2\" #>  #> $`_attn_implementation_autoset` #> [1] TRUE #>  #> $transformers_version #> [1] \"4.49.0\" #>  #> $model_type #> [1] \"gpt2\" #>  #> $n_ctx #> [1] 1024 #>"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp-deprecated.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp-deprecated","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp-deprecated","text":"function deprecated. Use causal_targets_pred() (supports l_context argument) causal_words_pred() (supports x arguments) instead.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp_mats-deprecated.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a list of matrices with the log probabilities of possible words given their previous context using a causal transformer — causal_lp_mats-deprecated","title":"Get a list of matrices with the log probabilities of possible words given their previous context using a causal transformer — causal_lp_mats-deprecated","text":"function deprecated. Use causal_pred_mats() instead.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_pred_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate next tokens after a context and their predictability using a causal transformer model — causal_next_tokens_pred_tbl","title":"Generate next tokens after a context and their predictability using a causal transformer model — causal_next_tokens_pred_tbl","text":"function predicts possible next tokens predictability (log-probabilities default). function sorts tokens descending order predictability.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_pred_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate next tokens after a context and their predictability using a causal transformer model — causal_next_tokens_pred_tbl","text":"","code":"causal_next_tokens_pred_tbl(   context,   log.p = getOption(\"pangoling.log.p\"),   decode = FALSE,   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_pred_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate next tokens after a context and their predictability using a causal transformer model — causal_next_tokens_pred_tbl","text":"context single string representing context next tokens predictabilities predicted. log.p Base logarithm used output predictability values. TRUE (default), natural logarithm (base e) used. FALSE, raw probabilities returned. Alternatively, log.p can set numeric value specifying base logarithm (e.g., 2 base-2 logarithms). get surprisal bits (rather predictability), set log.p = 1/2. decode Logical. TRUE, decodes tokens human-readable strings, handling special characters diacritics. Default FALSE. model Name pre-trained model folder. One able use models based \"gpt2\". See hugging face website. checkpoint Folder checkpoint. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_pred_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate next tokens after a context and their predictability using a causal transformer model — causal_next_tokens_pred_tbl","text":"table possible next tokens log-probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_pred_tbl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate next tokens after a context and their predictability using a causal transformer model — causal_next_tokens_pred_tbl","text":"function uses causal transformer model compute predictability tokens model's vocabulary, given single input context. returns table row represents token, along predictability score. default, function returns log-probabilities natural logarithm (base e), can specify different logarithm base (e.g., log.p = 1/2 surprisal bits). decode = TRUE, tokens converted human-readable strings, handling special characters like accents diacritics. ensures tokens interpretable, especially languages complex tokenization.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_pred_tbl.html","id":"more-details-about-causal-models","dir":"Reference","previous_headings":"","what":"More details about causal models","title":"Generate next tokens after a context and their predictability using a causal transformer model — causal_next_tokens_pred_tbl","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_pred_tbl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate next tokens after a context and their predictability using a causal transformer model — causal_next_tokens_pred_tbl","text":"","code":"causal_next_tokens_pred_tbl(   context = \"The apple doesn't fall far from the\",   model = \"gpt2\" ) #> Processing using causal model 'gpt2/' ... #> # A tidytable: 50,257 × 2 #>    token     pred #>    <chr>    <dbl> #>  1 Ġtree   -0.281 #>  2 Ġtrees  -3.60  #>  3 Ġapple  -4.29  #>  4 Ġtable  -4.50  #>  5 Ġhead   -4.83  #>  6 Ġmark   -4.86  #>  7 Ġcake   -4.91  #>  8 Ġground -5.08  #>  9 Ġtruth  -5.31  #> 10 Ġtop    -5.36  #> # ℹ 50,247 more rows"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_tbl-deprecated.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the possible next tokens and their log probabilities for its previous context — causal_next_tokens_tbl-deprecated","title":"Get the possible next tokens and their log probabilities for its previous context — causal_next_tokens_tbl-deprecated","text":"function deprecated. Use causal_next_tokens_pred_tbl() instead.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_pred_mats.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a list of predictability matrices using a causal transformer model — causal_pred_mats","title":"Generate a list of predictability matrices using a causal transformer model — causal_pred_mats","text":"function computes list matrices, matrix corresponds unique group specified argument. matrix represents predictability every token input text (x) based preceding context, evaluated causal transformer model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_pred_mats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a list of predictability matrices using a causal transformer model — causal_pred_mats","text":"","code":"causal_pred_mats(   x,   by = rep(1, length(x)),   sep = \" \",   log.p = getOption(\"pangoling.log.p\"),   sorted = FALSE,   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   decode = FALSE,   config_model = NULL,   config_tokenizer = NULL,   batch_size = 1,   ... )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_pred_mats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a list of predictability matrices using a causal transformer model — causal_pred_mats","text":"x character vector words, phrases, texts evaluate. grouping variable indicating texts split groups. sep string specifying words separated within contexts groups. Default \" \". languages spaces words (e.g., Chinese), set sep = \"\". log.p Base logarithm used output predictability values. TRUE (default), natural logarithm (base e) used. FALSE, raw probabilities returned. Alternatively, log.p can set numeric value specifying base logarithm (e.g., 2 base-2 logarithms). get surprisal bits (rather predictability), set log.p = 1/2. sorted default FALSE retain order groups splitting . TRUE sorted (according ) list(s) returned. model Name pre-trained model folder. One able use models based \"gpt2\". See hugging face website. checkpoint Folder checkpoint. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. decode Logical. TRUE, decodes tokens human-readable strings, handling special characters diacritics. Default FALSE. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed. batch_size Maximum number sentences/texts processed parallel. Larger batches increase speed use memory. Since texts batch must length, shorter ones padded placeholder tokens. ... Currently use.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_pred_mats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a list of predictability matrices using a causal transformer model — causal_pred_mats","text":"list matrices tokens columns vocabulary model rows","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_pred_mats.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a list of predictability matrices using a causal transformer model — causal_pred_mats","text":"function splits input x groups specified argument processes group independently. group, model computes predictability token vocabulary based preceding context. matrix contains: Rows representing model's vocabulary. Columns corresponding tokens group (e.g., sentence paragraph). default, values matrices natural logarithm word probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_pred_mats.html","id":"more-details-about-causal-models","dir":"Reference","previous_headings":"","what":"More details about causal models","title":"Generate a list of predictability matrices using a causal transformer model — causal_pred_mats","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_pred_mats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a list of predictability matrices using a causal transformer model — causal_pred_mats","text":"","code":"data(\"df_sent\") df_sent #> # A tidytable: 15 × 2 #>    sent_n word    #>     <int> <chr>   #>  1      1 The     #>  2      1 apple   #>  3      1 doesn't #>  4      1 fall    #>  5      1 far     #>  6      1 from    #>  7      1 the     #>  8      1 tree.   #>  9      2 Don't   #> 10      2 judge   #> 11      2 a       #> 12      2 book    #> 13      2 by      #> 14      2 its     #> 15      2 cover.  list_of_mats <- causal_pred_mats(                        x = df_sent$word,                        by = df_sent$sent_n,                          model = \"gpt2\"                 ) #> Processing using causal model 'gpt2/' ... #> Processing a batch of size 1 with 10 tokens. #> Processing a batch of size 1 with 9 tokens.  # View the structure of the resulting list list_of_mats |> str() #> List of 2 #>  $ 1: num [1:50257, 1:10] NA NA NA NA NA NA NA NA NA NA ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:50257] \"!\" \"\\\"\" \"#\" \"$\" ... #>   .. ..$ : chr [1:10] \"The\" \"Ġapple\" \"Ġdoesn\" \"'t\" ... #>  $ 2: num [1:50257, 1:9] NA NA NA NA NA NA NA NA NA NA ... #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:50257] \"!\" \"\\\"\" \"#\" \"$\" ... #>   .. ..$ : chr [1:9] \"Don\" \"'t\" \"Ġjudge\" \"Ġa\" ...  # Inspect the last rows of the first matrix list_of_mats[[1]] |> tail() #>               The     Ġapple     Ġdoesn        't     Ġfall      Ġfar     Ġfrom #> ominated       NA -15.142192 -19.096537 -32.80610 -28.53754 -25.08115 -27.33087 #> Ġregress       NA -13.093204 -14.924685 -32.17484 -12.01535 -20.19420 -21.34752 #> ĠCollider      NA -13.339488 -16.405062 -32.32004 -29.73265 -28.05914 -25.85092 #> Ġinformants    NA -12.950152 -14.873056 -34.56336 -24.34810 -18.73824 -23.02469 #> Ġgazed         NA -13.809768 -12.320757 -38.64273 -19.81046 -19.47638 -19.91819 #> <|endoftext|>  NA  -7.353133  -9.627634 -14.55747 -13.46111 -11.10686 -12.69976 #>                    Ġthe     Ġtree          . #> ominated      -20.71178 -21.72472 -22.202908 #> Ġregress      -18.08488 -16.63838 -18.050945 #> ĠCollider     -21.89429 -23.19034 -20.985302 #> Ġinformants   -18.23528 -18.46630 -17.959438 #> Ġgazed        -17.88233 -19.51767 -17.944866 #> <|endoftext|> -13.89558 -15.86696  -8.816073  # Inspect the last rows of the second matrix list_of_mats[[2]] |> tail() #>               Don         't    Ġjudge         Ġa     Ġbook       Ġby      Ġits #> ominated       NA -13.038710 -24.86986 -22.828133 -20.23553 -20.51216 -21.54437 #> Ġregress       NA -14.332914 -12.26382 -14.384551 -14.45043 -18.78388 -17.94085 #> ĠCollider      NA -15.067999 -16.55659 -18.564629 -17.95191 -20.40189 -22.59220 #> Ġinformants    NA -16.096315 -18.12209 -14.583617 -18.38805 -21.28853 -16.55606 #> Ġgazed         NA -15.309744 -18.21523 -18.643326 -17.84563 -20.48769 -17.91865 #> <|endoftext|>  NA  -6.514345 -10.62883  -9.849662 -12.23088 -10.47432 -12.96377 #>                  Ġcover          . #> ominated      -25.93669 -25.934177 #> Ġregress      -23.10072 -20.106548 #> ĠCollider     -25.76248 -25.129421 #> Ġinformants   -19.48792 -21.326099 #> Ġgazed        -22.67084 -20.240665 #> <|endoftext|> -16.37127  -8.178905"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_predictability.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute predictability using a causal transformer model — causal_words_pred","title":"Compute predictability using a causal transformer model — causal_words_pred","text":"functions calculate predictability words, phrases, tokens using causal transformer model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_predictability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute predictability using a causal transformer model — causal_words_pred","text":"","code":"causal_words_pred(   x,   by = rep(1, length(x)),   word_n = NULL,   sep = \" \",   log.p = getOption(\"pangoling.log.p\"),   ignore_regex = \"\",   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL,   batch_size = 1,   ... )  causal_tokens_pred_lst(   texts,   log.p = getOption(\"pangoling.log.p\"),   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL,   batch_size = 1 )  causal_targets_pred(   targets,   contexts = NULL,   sep = \" \",   log.p = getOption(\"pangoling.log.p\"),   ignore_regex = \"\",   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL,   batch_size = 1,   ... )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_predictability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute predictability using a causal transformer model — causal_words_pred","text":"x character vector words, phrases, texts evaluate. grouping variable indicating texts split groups. word_n Word order, default word order vector x. sep string specifying words separated within contexts groups. Default \" \". languages spaces words (e.g., Chinese), set sep = \"\". log.p Base logarithm used output predictability values. TRUE (default), natural logarithm (base e) used. FALSE, raw probabilities returned. Alternatively, log.p can set numeric value specifying base logarithm (e.g., 2 base-2 logarithms). get surprisal bits (rather predictability), set log.p = 1/2. ignore_regex Can ignore certain characters calculating log probabilities. example ^[[:punct:]]$ ignore punctuation  stands alone token. model Name pre-trained model folder. One able use models based \"gpt2\". See hugging face website. checkpoint Folder checkpoint. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed. batch_size Maximum number sentences/texts processed parallel. Larger batches increase speed use memory. Since texts batch must length, shorter ones padded placeholder tokens. ... Currently use. texts vector list sentences paragraphs. targets character vector target words phrases. contexts character vector contexts corresponding target.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_predictability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute predictability using a causal transformer model — causal_words_pred","text":"causal_targets_pred() causal_words_pred(), named numeric vector predictability scores. causal_tokens_pred_lst(), list named numeric vectors, one sentence group.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_predictability.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute predictability using a causal transformer model — causal_words_pred","text":"functions calculate predictability (default natural logarithm word probability) words, phrases tokens using causal transformer model: causal_targets_pred(): Evaluates specific target words phrases based given contexts. Use explicit context-target pairs evaluate, target word phrase paired single preceding context. causal_words_pred(): Computes predictability elements vector grouped specified variable. Use working words phrases split groups, sentences paragraphs, predictability computed every word phrase group. causal_tokens_pred_lst(): Computes predictability token sentence (group sentences) returns list results sentence. Use want calculate predictability every token one sentences. See online article pangoling website examples.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_predictability.html","id":"more-details-about-causal-models","dir":"Reference","previous_headings":"","what":"More details about causal models","title":"Compute predictability using a causal transformer model — causal_words_pred","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_predictability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute predictability using a causal transformer model — causal_words_pred","text":"","code":"# Using causal_targets_pred causal_targets_pred(   targets = c(\"tree.\", \"cover.\"),   contexts = c(\"The apple doesn't fall far from the\",                \"Don't judge a book by its\"),   model = \"gpt2\" ) #> Processing using causal model 'gpt2/' ... #> Processing a batch of size 1 with 10 tokens. #> Processing a batch of size 1 with 9 tokens. #> Text id: 1 #> `The apple doesn't fall far from the tree.` #> Text id: 2 #> `Don't judge a book by its cover.` #> *** #>     tree.    cover.  #> -1.581741 -1.377739   # Using causal_words_pred causal_words_pred(   x = df_sent$word,   by = df_sent$sent_n,   model = \"gpt2\" ) #> Processing using causal model 'gpt2/' ... #> Processing a batch of size 1 with 10 tokens. #> Processing a batch of size 1 with 9 tokens. #> Text id: 1 #> `The apple doesn't fall far from the tree.` #> Text id: 2 #> `Don't judge a book by its cover.` #> *** #>         The       apple     doesn't        fall         far        from  #>          NA -10.9004850  -5.4999222  -3.5977628  -2.9119270  -0.7454857  #>         the       tree.       Don't       judge           a        book  #>  -0.2066502  -1.5817409          NA  -6.2653966  -2.3259120  -1.9679886  #>          by         its      cover.  #>  -0.4091438  -0.2572804  -1.3777395   # Using causal_tokens_pred_lst preds <- causal_tokens_pred_lst(   texts = c(\"The apple doesn't fall far from the tree.\",             \"Don't judge a book by its cover.\"),   model = \"gpt2\" ) #> Processing using causal model 'gpt2/' ... #> Processing a batch of size 1 with 10 tokens. #> Processing a batch of size 1 with 9 tokens. preds #> [[1]] #>           The        Ġapple        Ġdoesn            't         Ġfall  #>            NA -1.090049e+01 -5.499094e+00 -8.281615e-04 -3.597763e+00  #>          Ġfar         Ġfrom          Ġthe         Ġtree             .  #> -2.911927e+00 -7.454857e-01 -2.066502e-01 -2.808041e-01 -1.300937e+00  #>  #> [[2]] #>         Don          't      Ġjudge          Ġa       Ġbook         Ġby  #>          NA -2.58639312 -6.26539660 -2.32591200 -1.96798861 -0.40914381  #>        Ġits      Ġcover           .  #> -0.25728044 -0.02360982 -1.35412967  #>   # Convert the output to a tidy table suppressPackageStartupMessages(library(tidytable)) map2_dfr(preds, seq_along(preds),  ~ data.frame(tokens = names(.x), pred = .x, id = .y)) #> # A tidytable: 19 × 3 #>    tokens       pred    id #>    <chr>       <dbl> <int> #>  1 The     NA            1 #>  2 Ġapple -10.9          1 #>  3 Ġdoesn  -5.50         1 #>  4 't      -0.000828     1 #>  5 Ġfall   -3.60         1 #>  6 Ġfar    -2.91         1 #>  7 Ġfrom   -0.745        1 #>  8 Ġthe    -0.207        1 #>  9 Ġtree   -0.281        1 #> 10 .       -1.30         1 #> 11 Don     NA            2 #> 12 't      -2.59         2 #> 13 Ġjudge  -6.27         2 #> 14 Ġa      -2.33         2 #> 15 Ġbook   -1.97         2 #> 16 Ġby     -0.409        2 #> 17 Ġits    -0.257        2 #> 18 Ġcover  -0.0236       2 #> 19 .       -1.35         2"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":null,"dir":"Reference","previous_headings":"","what":"Preloads a causal language model — causal_preload","title":"Preloads a causal language model — causal_preload","text":"Preloads causal language model speed next runs.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preloads a causal language model — causal_preload","text":"","code":"causal_preload(   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preloads a causal language model — causal_preload","text":"model Name pre-trained model folder. One able use models based \"gpt2\". See hugging face website. checkpoint Folder checkpoint. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preloads a causal language model — causal_preload","text":"Nothing.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"more-details-about-causal-models","dir":"Reference","previous_headings":"","what":"More details about causal models","title":"Preloads a causal language model — causal_preload","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preloads a causal language model — causal_preload","text":"","code":"causal_preload(model = \"gpt2\") #> Preloading causal model gpt2..."},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_tokens_lp_tbl-deprecated.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl-deprecated","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl-deprecated","text":"function deprecated. Use causal_tokens_pred_lst() instead.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_jaeger14.html","id":null,"dir":"Reference","previous_headings":"","what":"Self-Paced Reading Dataset on Chinese Relative Clauses — df_jaeger14","title":"Self-Paced Reading Dataset on Chinese Relative Clauses — df_jaeger14","text":"dataset contains data self-paced reading experiment Chinese relative clause comprehension. structured support analysis reaction times, comprehension accuracy, surprisal values across various experimental conditions 2x2 fully crossed factorial design:","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_jaeger14.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Self-Paced Reading Dataset on Chinese Relative Clauses — df_jaeger14","text":"","code":"data(df_jaeger14)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_jaeger14.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Self-Paced Reading Dataset on Chinese Relative Clauses — df_jaeger14","text":"tibble 8,624 rows 15 variables: subject Participant identifier, character vector. item Trial item number, integer. cond Experimental condition, character vector indicating variations sentence structure (e.g., \"\", \"b\", \"c\", \"d\"). word Chinese word presented trial, character vector. wordn Position word within sentence, integer. rt Reaction time milliseconds reading word, integer. region Sentence region phrase type (e.g., \"hd1\", \"Det+CL\"), character vector. question Comprehension question associated trial, character vector. accuracy Binary accuracy score comprehension question (1 = correct, 0 = incorrect). correct_answer Expected correct answer comprehension question, character vector (\"Y\" \"N\"). question_type Type comprehension question, character vector. experiment Name experiment, indicating self-paced reading, character vector. list Experimental list number, counterbalancing item presentation, integer. sentence Full sentence used trial words marked analysis, character vector. surprisal Model-derived surprisal values word, numeric vector. Region codes dataset (column region): N: Main clause subject (object-modifications ) V: Main clause verb (object-modifications ) Det+CL: Determiner+classifier Adv: Adverb VN: RC-verb+RC-object (subject relatives) RC-subject+RC-verb (object relatives) Note: two words merged one region experiment; presented separate regions experiment. FreqP: Frequency phrase/durational phrase DE: Relativizer \"de\" head: Relative clause head noun hd1: First word head noun hd2: Second word head noun hd3: Third word head noun hd4: Fourth word head noun (subject-modifications) hd5: Fifth word head noun (subject-modifications) Notes reading times (column rt): reading time relative clause region (e.g., \"V-N\" \"N-V\") computed summing reading times relative clause verb noun. verb noun presented two separate regions experiment.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_jaeger14.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Self-Paced Reading Dataset on Chinese Relative Clauses — df_jaeger14","text":"Jäger, L., Chen, Z., Li, Q., Lin, C.-J. C., & Vasishth, S. (2015). subject-relative advantage Chinese: Evidence expectation-based processing. Journal Memory Language, 79–80, 97-120. https://doi.org/10.1016/j.jml.2014.10.005","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_jaeger14.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Self-Paced Reading Dataset on Chinese Relative Clauses — df_jaeger14","text":"Factor : Modification type (subject modification; object modification) Factor II: Relative clause type (subject relative; object relative) Condition labels: ) subject modification; subject relative b) subject modification; object relative c) object modification; subject relative d) object modification; object relative","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_jaeger14.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Self-Paced Reading Dataset on Chinese Relative Clauses — df_jaeger14","text":"","code":"# Basic exploration head(df_jaeger14) #> # A tidytable: 6 × 14 #>   subject  item cond  word   wordn    rt region question accuracy correct_answer #>   <chr>   <int> <chr> <chr>  <int> <int> <fct>  <chr>       <int>          <int> #> 1 1m1         1 a     那個       1   360 Det+CL 那個顧客聽說過…        1              1 #> 2 1m1         1 a     昨晚       2   359 Adv    那個顧客聽說過…        1              1 #> 3 1m1         1 a     揍了服務生…     3   344 VN     那個顧客聽說過…        1              1 #> 4 1m1         1 a     一頓       4   313 FreqP  那個顧客聽說過…        1              1 #> 5 1m1         1 a     的         5   297 DE     那個顧客聽說過…        1              1 #> 6 1m1         1 a     顧客       6   312 head   那個顧客聽說過…        1              1 #> # ℹ 4 more variables: question_type <int>, experiment <chr>, list <int>, #> #   sentence <chr>  # Summarize reaction times by region  library(tidytable) df_jaeger14 |>   group_by(region) |>   summarize(mean_rt = mean(rt, na.rm = TRUE)) #> # A tidytable: 13 × 2 #>    region mean_rt #>    <fct>    <dbl> #>  1 N         614. #>  2 V         538. #>  3 Det+CL    513. #>  4 Adv       541. #>  5 VN        618. #>  6 FreqP     603. #>  7 DE        439. #>  8 head      653. #>  9 hd1       604. #> 10 hd2       538. #> 11 hd3       704. #> 12 hd4       479. #> 13 hd5       865."},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_sent.html","id":null,"dir":"Reference","previous_headings":"","what":"Example dataset: Two word-by-word tokenized sentences — df_sent","title":"Example dataset: Two word-by-word tokenized sentences — df_sent","text":"dataset contains tokenized words two example sentences, split word--word. structured demonstrate use pangoling package processing text data. package processing text data.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_sent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example dataset: Two word-by-word tokenized sentences — df_sent","text":"","code":"df_sent"},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_sent.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example dataset: Two word-by-word tokenized sentences — df_sent","text":"data frame 15 rows 2 columns: sent_n (integer) Sentence number, indicating sentence word belongs . word (character) Tokenized words sentences.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/df_sent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Example dataset: Two word-by-word tokenized sentences — df_sent","text":"","code":"# Load the dataset data(\"df_sent\") df_sent #> # A tidytable: 15 × 2 #>    sent_n word    #>     <int> <chr>   #>  1      1 The     #>  2      1 apple   #>  3      1 doesn't #>  4      1 fall    #>  5      1 far     #>  6      1 from    #>  7      1 the     #>  8      1 tree.   #>  9      2 Don't   #> 10      2 judge   #> 11      2 a       #> 12      2 book    #> 13      2 by      #> 14      2 its     #> 15      2 cover."},{"path":"http://bruno.nicenboim.me/pangoling/reference/install_py_pangoling.html","id":null,"dir":"Reference","previous_headings":"","what":"Install the Python packages needed for pangoling — install_py_pangoling","title":"Install the Python packages needed for pangoling — install_py_pangoling","text":"install_py_pangoling function facilitates installation Python packages needed using pangoling within R environment, utilizing reticulate package managing Python environments. supports various installation methods, environment settings, Python versions.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/install_py_pangoling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install the Python packages needed for pangoling — install_py_pangoling","text":"","code":"install_py_pangoling(method = c(\"auto\", \"virtualenv\", \"conda\"),                      conda = \"auto\",                      version = \"default\",                      envname = \"r-pangoling\",                      restart_session = TRUE,                      conda_python_version = NULL,                      ...,                      pip_ignore_installed = FALSE,                      new_env = identical(envname, \"r-pangoling\"),                      python_version = NULL)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/install_py_pangoling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install the Python packages needed for pangoling — install_py_pangoling","text":"method character vector specifying environment management method. Options 'auto', 'virtualenv', 'conda'. Default 'auto'. conda Specifies conda binary use. Default 'auto'. version Python version use. Default 'default', automatically selected. envname Name virtual environment. Default 'r-pangoling'. restart_session Logical, whether restart R session installation. Default TRUE. conda_python_version Python version conda environments. ... Additional arguments passed reticulate::py_install. pip_ignore_installed Logical, whether ignore already installed packages. Default FALSE. new_env Logical, whether create new environment envname 'r-pangoling'. Default identity envname. python_version Specifies Python version environment.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/install_py_pangoling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Install the Python packages needed for pangoling — install_py_pangoling","text":"function returns NULL invisibly, outputs message successful installation.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/install_py_pangoling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Install the Python packages needed for pangoling — install_py_pangoling","text":"function automatically selects appropriate method environment management Python installation, focus virtual conda environments. ensures flexibility dependency management Python version control. new environment created, existing environments name removed.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/install_py_pangoling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Install the Python packages needed for pangoling — install_py_pangoling","text":"","code":"if (FALSE) { # \\dontrun{ # Install with default settings: install_py_pangoling() } # }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the configuration of a masked model — masked_config","title":"Returns the configuration of a masked model — masked_config","text":"Returns configuration masked model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the configuration of a masked model — masked_config","text":"","code":"masked_config(   model = getOption(\"pangoling.masked.default\"),   config_model = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the configuration of a masked model — masked_config","text":"model Name pre-trained model folder. One able use models based \"bert\". See hugging face website. config_model List arguments control model Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the configuration of a masked model — masked_config","text":"list configuration model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Returns the configuration of a masked model — masked_config","text":"masked language model (also called BERT-like, encoder model) type large language model  can used predict content mask sentence. specified, masked model used one set specified global option pangoling.masked.default, can accessed via getOption(\"pangoling.masked.default\") (default \"bert-base-uncased\"). change default option use options(pangoling.masked.default = \"newmaskedmodel\"). list possible masked can found Hugging Face website Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see python method from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the configuration of a masked model — masked_config","text":"","code":"masked_config(model = \"bert-base-uncased\") #> $return_dict #> [1] TRUE #>  #> $output_hidden_states #> [1] FALSE #>  #> $output_attentions #> [1] FALSE #>  #> $torchscript #> [1] FALSE #>  #> $torch_dtype #> [1] \"float32\" #>  #> $use_bfloat16 #> [1] FALSE #>  #> $tf_legacy_loss #> [1] FALSE #>  #> $pruned_heads #> named list() #>  #> $tie_word_embeddings #> [1] TRUE #>  #> $chunk_size_feed_forward #> [1] 0 #>  #> $is_encoder_decoder #> [1] FALSE #>  #> $is_decoder #> [1] FALSE #>  #> $cross_attention_hidden_size #> NULL #>  #> $add_cross_attention #> [1] FALSE #>  #> $tie_encoder_decoder #> [1] FALSE #>  #> $max_length #> [1] 20 #>  #> $min_length #> [1] 0 #>  #> $do_sample #> [1] FALSE #>  #> $early_stopping #> [1] FALSE #>  #> $num_beams #> [1] 1 #>  #> $num_beam_groups #> [1] 1 #>  #> $diversity_penalty #> [1] 0 #>  #> $temperature #> [1] 1 #>  #> $top_k #> [1] 50 #>  #> $top_p #> [1] 1 #>  #> $typical_p #> [1] 1 #>  #> $repetition_penalty #> [1] 1 #>  #> $length_penalty #> [1] 1 #>  #> $no_repeat_ngram_size #> [1] 0 #>  #> $encoder_no_repeat_ngram_size #> [1] 0 #>  #> $bad_words_ids #> NULL #>  #> $num_return_sequences #> [1] 1 #>  #> $output_scores #> [1] FALSE #>  #> $return_dict_in_generate #> [1] \"TRUE\" #>  #> $forced_bos_token_id #> NULL #>  #> $forced_eos_token_id #> NULL #>  #> $remove_invalid_values #> [1] FALSE #>  #> $exponential_decay_length_penalty #> NULL #>  #> $suppress_tokens #> NULL #>  #> $begin_suppress_tokens #> NULL #>  #> $architectures #> [1] \"BertForMaskedLM\" #>  #> $finetuning_task #> NULL #>  #> $id2label #> $id2label$`0` #> [1] \"LABEL_0\" #>  #> $id2label$`1` #> [1] \"LABEL_1\" #>  #>  #> $label2id #> $label2id$LABEL_0 #> [1] 0 #>  #> $label2id$LABEL_1 #> [1] 1 #>  #>  #> $tokenizer_class #> NULL #>  #> $prefix #> NULL #>  #> $bos_token_id #> NULL #>  #> $pad_token_id #> [1] 0 #>  #> $eos_token_id #> NULL #>  #> $sep_token_id #> NULL #>  #> $decoder_start_token_id #> NULL #>  #> $task_specific_params #> NULL #>  #> $problem_type #> NULL #>  #> $`_name_or_path` #> [1] \"bert-base-uncased\" #>  #> $`_attn_implementation_autoset` #> [1] TRUE #>  #> $transformers_version #> [1] \"4.49.0\" #>  #> $gradient_checkpointing #> [1] FALSE #>  #> $model_type #> [1] \"bert\" #>  #> $vocab_size #> [1] 30522 #>  #> $hidden_size #> [1] 768 #>  #> $num_hidden_layers #> [1] 12 #>  #> $num_attention_heads #> [1] 12 #>  #> $hidden_act #> [1] \"gelu\" #>  #> $intermediate_size #> [1] 3072 #>  #> $hidden_dropout_prob #> [1] 0.1 #>  #> $attention_probs_dropout_prob #> [1] 0.1 #>  #> $max_position_embeddings #> [1] 512 #>  #> $type_vocab_size #> [1] 2 #>  #> $initializer_range #> [1] 0.02 #>  #> $layer_norm_eps #> [1] 1e-12 #>  #> $position_embedding_type #> [1] \"absolute\" #>  #> $use_cache #> [1] TRUE #>  #> $classifier_dropout #> NULL #>"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_lp-deprecated.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp-deprecated","title":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp-deprecated","text":"function deprecated. Use masked_targets_pred() instead.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":null,"dir":"Reference","previous_headings":"","what":"Preloads a masked language model — masked_preload","title":"Preloads a masked language model — masked_preload","text":"Preloads masked language model speed next runs.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preloads a masked language model — masked_preload","text":"","code":"masked_preload(   model = getOption(\"pangoling.masked.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preloads a masked language model — masked_preload","text":"model Name pre-trained model folder. One able use models based \"bert\". See hugging face website. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preloads a masked language model — masked_preload","text":"Nothing.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preloads a masked language model — masked_preload","text":"masked language model (also called BERT-like, encoder model) type large language model  can used predict content mask sentence. specified, masked model used one set specified global option pangoling.masked.default, can accessed via getOption(\"pangoling.masked.default\") (default \"bert-base-uncased\"). change default option use options(pangoling.masked.default = \"newmaskedmodel\"). list possible masked can found Hugging Face website Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see python method from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preloads a masked language model — masked_preload","text":"","code":"causal_preload(model = \"bert-base-uncased\") #> Preloading causal model bert-base-uncased..."},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_targets_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the predictability of a target word (or phrase) given a left and right context — masked_targets_pred","title":"Get the predictability of a target word (or phrase) given a left and right context — masked_targets_pred","text":"Get predictability (default natural logarithm word probability) vector target words (phrase) given vector left right contexts using masked transformer.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_targets_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the predictability of a target word (or phrase) given a left and right context — masked_targets_pred","text":"","code":"masked_targets_pred(   prev_contexts,   targets,   after_contexts,   log.p = getOption(\"pangoling.log.p\"),   ignore_regex = \"\",   model = getOption(\"pangoling.masked.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_targets_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the predictability of a target word (or phrase) given a left and right context — masked_targets_pred","text":"prev_contexts Left context target word left--right written languages. targets Target words. after_contexts Right context target left--right written languages. log.p Base logarithm used output predictability values. TRUE (default), natural logarithm (base e) used. FALSE, raw probabilities returned. Alternatively, log.p can set numeric value specifying base logarithm (e.g., 2 base-2 logarithms). get surprisal bits (rather predictability), set log.p = 1/2. ignore_regex Can ignore certain characters calculating log probabilities. example ^[[:punct:]]$ ignore punctuation  stands alone token. model Name pre-trained model folder. One able use models based \"bert\". See hugging face website. checkpoint Folder checkpoint. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_targets_pred.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the predictability of a target word (or phrase) given a left and right context — masked_targets_pred","text":"named vector predictability values (default natural logarithm word probability).","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_targets_pred.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the predictability of a target word (or phrase) given a left and right context — masked_targets_pred","text":"masked language model (also called BERT-like, encoder model) type large language model  can used predict content mask sentence. specified, masked model used one set specified global option pangoling.masked.default, can accessed via getOption(\"pangoling.masked.default\") (default \"bert-base-uncased\"). change default option use options(pangoling.masked.default = \"newmaskedmodel\"). list possible masked can found Hugging Face website Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see python method from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_targets_pred.html","id":"more-examples","dir":"Reference","previous_headings":"","what":"More examples","title":"Get the predictability of a target word (or phrase) given a left and right context — masked_targets_pred","text":"See online article pangoling website examples.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_targets_pred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the predictability of a target word (or phrase) given a left and right context — masked_targets_pred","text":"","code":"masked_targets_pred(   prev_contexts = c(\"The\", \"The\"),   targets = c(\"apple\", \"pear\"),   after_contexts = c(     \"doesn't fall far from the tree.\",     \"doesn't fall far from the tree.\"   ),   model = \"bert-base-uncased\" ) #> Processing using masked model 'bert-base-uncased/' ... #> Processing 1 batch(es) of 13 tokens. #> The [apple] doesn't fall far from the tree. #> Processing 1 batch(es) of 13 tokens. #> The [pear] doesn't fall far from the tree. #> *** #>     apple      pear  #> -4.681238 -8.603889"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_pred_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_pred_tbl","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_pred_tbl","text":"mask, indicated [MASK], sentence, get possible tokens  predictability (default natural logarithm word probability) using masked transformer.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_pred_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_pred_tbl","text":"","code":"masked_tokens_pred_tbl(   masked_sentences,   log.p = getOption(\"pangoling.log.p\"),   model = getOption(\"pangoling.masked.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_pred_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_pred_tbl","text":"masked_sentences Masked sentences. log.p Base logarithm used output predictability values. TRUE (default), natural logarithm (base e) used. FALSE, raw probabilities returned. Alternatively, log.p can set numeric value specifying base logarithm (e.g., 2 base-2 logarithms). get surprisal bits (rather predictability), set log.p = 1/2. model Name pre-trained model folder. One able use models based \"bert\". See hugging face website. checkpoint Folder checkpoint. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_pred_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_pred_tbl","text":"table masked sentences, tokens (token), predictability (pred), respective mask number (mask_n).","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_pred_tbl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_pred_tbl","text":"masked language model (also called BERT-like, encoder model) type large language model  can used predict content mask sentence. specified, masked model used one set specified global option pangoling.masked.default, can accessed via getOption(\"pangoling.masked.default\") (default \"bert-base-uncased\"). change default option use options(pangoling.masked.default = \"newmaskedmodel\"). list possible masked can found Hugging Face website Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see python method from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_pred_tbl.html","id":"more-examples","dir":"Reference","previous_headings":"","what":"More examples","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_pred_tbl","text":"See online article pangoling website examples.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_pred_tbl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_pred_tbl","text":"","code":"masked_tokens_pred_tbl(\"The [MASK] doesn't fall far from the tree.\",   model = \"bert-base-uncased\" ) #> Processing using masked model 'bert-base-uncased/' ... #> # A tidytable: 30,522 × 4 #>    masked_sentence                            token   pred mask_n #>    <chr>                                      <chr>  <dbl>  <int> #>  1 The [MASK] doesn't fall far from the tree. snow   -3.12      1 #>  2 The [MASK] doesn't fall far from the tree. rock   -3.45      1 #>  3 The [MASK] doesn't fall far from the tree. stone  -3.77      1 #>  4 The [MASK] doesn't fall far from the tree. girl   -3.85      1 #>  5 The [MASK] doesn't fall far from the tree. sun    -3.99      1 #>  6 The [MASK] doesn't fall far from the tree. tree   -3.99      1 #>  7 The [MASK] doesn't fall far from the tree. branch -4.09      1 #>  8 The [MASK] doesn't fall far from the tree. body   -4.12      1 #>  9 The [MASK] doesn't fall far from the tree. light  -4.43      1 #> 10 The [MASK] doesn't fall far from the tree. water  -4.44      1 #> # ℹ 30,512 more rows"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_tbl-deprecated.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl-deprecated","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl-deprecated","text":"function deprecated. Use masked_tokens_pred_tbl() instead.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":null,"dir":"Reference","previous_headings":"","what":"The number of tokens in a string or vector of strings — ntokens","title":"The number of tokens in a string or vector of strings — ntokens","text":"number tokens string vector strings","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The number of tokens in a string or vector of strings — ntokens","text":"","code":"ntokens(   x,   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The number of tokens in a string or vector of strings — ntokens","text":"x character input model Name pre-trained model folder. One able use models based \"gpt2\". See hugging face website. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The number of tokens in a string or vector of strings — ntokens","text":"number tokens string vector words.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The number of tokens in a string or vector of strings — ntokens","text":"","code":"ntokens(x = c(\"The apple doesn't fall far from the tree.\"), model = \"gpt2\") #> [1] 10"},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-deprecated.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated functions in package pangoling. — pangoling-deprecated","title":"Deprecated functions in package pangoling. — pangoling-deprecated","text":"functions listed deprecated defunct near future. possible, alternative functions similar functionality also mentioned. Help pages deprecated functions available help(\"<function>-deprecated\").","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-deprecated.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated functions in package pangoling. — pangoling-deprecated","text":"","code":"masked_tokens_tbl(   masked_sentences,   model = getOption(\"pangoling.masked.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )  masked_lp(   l_contexts,   targets,   r_contexts,   ignore_regex = \"\",   model = getOption(\"pangoling.masked.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )  causal_next_tokens_tbl(   context,   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )  causal_lp(   x,   by = rep(1, length(x)),   l_contexts = NULL,   ignore_regex = \"\",   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL,   batch_size = 1,   ... )  causal_tokens_lp_tbl(   texts,   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL,   batch_size = 1,   .id = NULL )  causal_lp_mats(   x,   by = rep(1, length(x)),   sorted = FALSE,   model = getOption(\"pangoling.causal.default\"),   checkpoint = NULL,   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL,   batch_size = 1,   ... )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-deprecated.html","id":"masked-tokens-tbl","dir":"Reference","previous_headings":"","what":"masked_tokens_tbl","title":"Deprecated functions in package pangoling. — pangoling-deprecated","text":"masked_tokens_tbl, use masked_tokens_pred_tbl.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-deprecated.html","id":"masked-lp","dir":"Reference","previous_headings":"","what":"masked_lp","title":"Deprecated functions in package pangoling. — pangoling-deprecated","text":"masked_lp, use masked_targets_pred.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-deprecated.html","id":"causal-next-tokens-tbl","dir":"Reference","previous_headings":"","what":"causal_next_tokens_tbl","title":"Deprecated functions in package pangoling. — pangoling-deprecated","text":"causal_next_tokens_tbl, use causal_next_tokens_pred_tbl.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-deprecated.html","id":"causal-lp","dir":"Reference","previous_headings":"","what":"causal_lp","title":"Deprecated functions in package pangoling. — pangoling-deprecated","text":"causal_lp, use causal_targets_pred causal_words_pred.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-deprecated.html","id":"causal-tokens-lp-tbl","dir":"Reference","previous_headings":"","what":"causal_tokens_lp_tbl","title":"Deprecated functions in package pangoling. — pangoling-deprecated","text":"causal_tokens_lp_tbl, use causal_tokens_pred_lst.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-deprecated.html","id":"causal-lp-mats","dir":"Reference","previous_headings":"","what":"causal_lp_mats","title":"Deprecated functions in package pangoling. — pangoling-deprecated","text":"causal_lp_mats, use causal_pred_mats.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-package.html","id":null,"dir":"Reference","previous_headings":"","what":"pangoling: Access to Large Language Model Predictions — pangoling-package","title":"pangoling: Access to Large Language Model Predictions — pangoling-package","text":"Access word predictability using large language (transformer) models.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"pangoling: Access to Large Language Model Predictions — pangoling-package","text":"options used control various aspects pangoling package. Users can customize options via options() function specifying pangoling.<option> names. pangoling.debug: Logical; TRUE, enables debugging mode. Default FALSE. pangoling.verbose: Integer; controls verbosity level (e.g., 0 = silent, 1 = minimal, 2 = detailed). Default 2. pangoling.log.p: Logical; TRUE (default), pangoling outputs log-transformed probabilities base e, FALSE output raw probabilities. Alternatively log.p can base logarithmic transformations (e.g., base 1/2, get surprisal values bits rather predictability). pangoling.cache: cache object created cachem::cache_mem, allowing specify maximum size (bytes) cached objects. Default 1024 * 1024^2 bytes (1 MB). pangoling.causal.default: Character string; specifies default model causal language processing. Default \"gpt2\". pangoling.masked.default: Character string; specifies default model masked language processing. Default \"bert-base-uncased\". Use options(pangoling.<option> = <value>) set options session.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"pangoling: Access to Large Language Model Predictions — pangoling-package","text":"Maintainer: Bruno Nicenboim bruno.nicenboim@gmail.com (ORCID) contributors: Chris Emmerly [contributor] Giovanni Cassani [contributor] Lisa Levinson [reviewer] Utku Turk [reviewer]","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"pangoling: Access to Large Language Model Predictions — pangoling-package","text":"","code":"if (FALSE) { # interactive() options(pangoling.verbose = FALSE) # Removes messages }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates perplexity — perplexity_calc","title":"Calculates perplexity — perplexity_calc","text":"Calculates perplexity vector (log-)probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates perplexity — perplexity_calc","text":"","code":"perplexity_calc(x, na.rm = FALSE, log.p = TRUE)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates perplexity — perplexity_calc","text":"x vector log-probabilities. na.rm missing values (including NaN) removed? log.p TRUE (default),  x assumed log-transformed probabilities base e, FALSE x assumed raw probabilities, alternatively log.p can base logarithmic transformations.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates perplexity — perplexity_calc","text":"perplexity.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculates perplexity — perplexity_calc","text":"x raw probabilities (default), perplexity calculated follows: $$\\left(\\prod_{n} x_n \\right)^\\frac{1}{N}$$","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculates perplexity — perplexity_calc","text":"","code":"probs <- c(.3, .5, .6) perplexity_calc(probs, log.p = FALSE) #> [1] 2.231443 lprobs <- log(probs) perplexity_calc(lprobs, log.p = TRUE) #> [1] 2.231443"},{"path":"http://bruno.nicenboim.me/pangoling/reference/set_cache_folder.html","id":null,"dir":"Reference","previous_headings":"","what":"Set cache folder for HuggingFace transformers — set_cache_folder","title":"Set cache folder for HuggingFace transformers — set_cache_folder","text":"function sets cache directory HuggingFace transformers. path given, function checks directory exists sets TRANSFORMERS_CACHE environment variable path. path provided, function checks existing cache directory number environment variables. none environment variables set, provides user information default cache directory.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/set_cache_folder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set cache folder for HuggingFace transformers — set_cache_folder","text":"","code":"set_cache_folder(path = NULL)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/set_cache_folder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set cache folder for HuggingFace transformers — set_cache_folder","text":"path Character string, path set cache directory. NULL, function look cache directory number environment variables. Default NULL.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/set_cache_folder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set cache folder for HuggingFace transformers — set_cache_folder","text":"Nothing returned, function called side effect setting TRANSFORMERS_CACHE environment variable, providing information user.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/set_cache_folder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set cache folder for HuggingFace transformers — set_cache_folder","text":"","code":"if (FALSE) { # \\dontrun{ set_cache_folder(\"~/new_cache_dir\") } # }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize an input — tokenize_lst","title":"Tokenize an input — tokenize_lst","text":"Tokenize string token ids.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize an input — tokenize_lst","text":"","code":"tokenize_lst(   x,   decode = FALSE,   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize an input — tokenize_lst","text":"x Strings token ids. decode Logical. TRUE, decodes tokens human-readable strings, handling special characters diacritics. Default FALSE. model Name pre-trained model folder. One able use models based \"gpt2\". See hugging face website. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize an input — tokenize_lst","text":"list tokens","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize an input — tokenize_lst","text":"","code":"tokenize_lst(x = c(\"The apple doesn't fall far from the tree.\"),               model = \"gpt2\") #> [[1]] #>  [1] \"The\"    \"Ġapple\" \"Ġdoesn\" \"'t\"     \"Ġfall\"  \"Ġfar\"   \"Ġfrom\"  \"Ġthe\"   #>  [9] \"Ġtree\"  \".\"      #>"},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the vocabulary of a model — transformer_vocab","title":"Returns the vocabulary of a model — transformer_vocab","text":"Returns (decoded) vocabulary model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the vocabulary of a model — transformer_vocab","text":"","code":"transformer_vocab(   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   decode = FALSE,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the vocabulary of a model — transformer_vocab","text":"model Name pre-trained model folder. One able use models based \"gpt2\". See hugging face website. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. decode Logical. TRUE, decodes tokens human-readable strings, handling special characters diacritics. Default FALSE. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the vocabulary of a model — transformer_vocab","text":"vector vocabulary model.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the vocabulary of a model — transformer_vocab","text":"","code":"transformer_vocab(model = \"gpt2\") |>  head() #> [1] \"!\"  \"\\\"\" \"#\"  \"$\"  \"%\"  \"&\""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009000","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9000","title":"pangoling 0.0.0.9000","text":"First release!","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009001","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9001","title":"pangoling 0.0.0.9001","text":"Tons stuff. Fully functional package now.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009002","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9002","title":"pangoling 0.0.0.9002","text":"minor function names avoid conflict packages","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009003","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9003","title":"pangoling 0.0.0.9003","text":"bug causal_tokens_lp_tbl fixed","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009004","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9004","title":"pangoling 0.0.0.9004","text":"Causal models accept batches.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009005","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9005","title":"pangoling 0.0.0.9005","text":"Strings tokens longer throw errors. Requires correct version R.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009006","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9006","title":"pangoling 0.0.0.9006","text":"causal_lp get l_contexts argument. Checkpoints work causal models (yet masked models). Ropensci badge added.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009007","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9007","title":"pangoling 0.0.0.9007","text":"set_cache_folder() function added. Message package loads. New troubleshooting vignette.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009008","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9008","title":"pangoling 0.0.0.9008","text":"Fix bug .unordered","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009009","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9009","title":"pangoling 0.0.0.9009","text":"Deprecated .favor .","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"new-features-0-0-0-9010","dir":"Changelog","previous_headings":"","what":"New Features:","title":"pangoling 0.0.0.9010","text":"Added checkpoint parameter causal_preload() masked_preload() allow loading models checkpoints. Introduced causal_next_tokens_pred_tbl(), replaces causal_next_tokens_tbl() provides improved predictability calculations. Added causal_words_pred(), causal_targets_pred(), causal_tokens_pred_lst() compute predictability words, phrases, tokens, replacing causal_lp() causal_tokens_lp_tbl(). Introduced masked_tokens_pred_tbl(), replacing masked_tokens_tbl(), retrieving possible tokens log probabilities. Introduced masked_targets_pred(), replacing masked_lp(), calculating predictability based left right context. Introduced transformer_vocab() optional decode parameter return decoded tokenized words. New dataset df_jaeger14: Self-paced reading data Chinese relative clauses. New dataset df_sent: Example dataset two word--word tokenized sentences. New vignette: Added worked-example causal model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"enhancements-0-0-0-9010","dir":"Changelog","previous_headings":"","what":"Enhancements:","title":"pangoling 0.0.0.9010","text":"Added sep argument causal_words_pred() support languages without spaces words (e.g., Chinese). New log.p argument across multiple functions specify predictability calculated (e.g., log base e, log base 2 bits, raw probabilities). Improved tokenization utilities: tokenize_lst() now supports decoded outputs via decode parameter. Updated install_py_pangoling() enhance Python environment handling. Added perplexity_calc() computing perplexity probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"deprecations-0-0-0-9010","dir":"Changelog","previous_headings":"","what":"Deprecations:","title":"pangoling 0.0.0.9010","text":"Deprecated causal_next_tokens_tbl(), causal_lp(), causal_tokens_lp_tbl(), causal_lp_mats(). Use causal_next_tokens_pred_tbl(), causal_targets_pred(), causal_words_pred(), causal_pred_mats() instead. Deprecated masked_tokens_tbl() masked_lp(). Use masked_tokens_pred_tbl() masked_targets_pred() instead.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009011","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9011","title":"pangoling 0.0.0.9011","text":"Added word_n argument causal_words_pred() indicate word order teh texts. Allows models larger vocabulary tokenizer.","code":""}]
