[{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to pangoling","title":"Contributing to pangoling","text":"outlines propose change pangoling. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to pangoling","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to pangoling","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed).","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to pangoling","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"bnicenboim/pangoling\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to pangoling","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to pangoling","text":"Please note pangoling project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 Bruno Nicenboim Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bruno Nicenboim. Author, maintainer. Chris Emmerly. Contributor. Giovanni Cassani. Contributor.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Nicenboim B (2023). pangoling: Access large language model predictions R. doi:10.5281/zenodo.7637526, R package version 0.0.0.9002, https://github.com/bnicenboim/pangoling.","code":"@Manual{,   title = {{pangoling}: {Access} to large language model predictions in {R}},   author = {Bruno Nicenboim},   year = {2023},   note = {R package version 0.0.0.9002},   doi = {10.5281/zenodo.7637526},   url = {https://github.com/bnicenboim/pangoling}, }"},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"pangoling-","dir":"","previous_headings":"","what":"Access to Large Language Model Predictions in R","title":"Access to Large Language Model Predictions in R","text":"pangoling1 R package estimating log-probabilities words given context using transformer models. package provides interface utilizing pre-trained transformer models (GPT-2 BERT) obtain word probabilities. log-probabilities often utilized predictors psycholinguistic studies. package can useful researchers field psycholinguistics want leverage power transformer models work. package mostly wrapper python package transformers process data convenient format.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"important-limitations-and-bias","dir":"","previous_headings":"","what":"Important! Limitations and bias","title":"Access to Large Language Model Predictions in R","text":"training data popular models (GPT-2) haven’t released, one inspect . ’s clear data contain lot unfiltered content internet, far neutral. See example scope openAI team’s model card GPT-2, many models, limitations bias section GPT-2 Hugging Face website.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Access to Large Language Model Predictions in R","text":"still released version pangoling. package ** early** stages development, probably subject changes. install latest version github use:","code":"# install.packages(\"remotes\") # if needed remotes::install_github(\"bnicenboim/pangoling\")"},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Access to Large Language Model Predictions in R","text":"basic example shows get log-probabilities words dataset: Given (toy) dataset sentences organized one word short phrase row: One can get log-transformed probability word based GPT-2 follows:","code":"library(pangoling) library(tidytable) #fast alternative to dplyr sentences <- c(\"The apple doesn't fall far from the tree.\",                 \"Don't judge a book by its cover.\") (df_sent <- strsplit(x = sentences, split = \" \") |>    map_dfr(.f =  ~ data.frame(word = .x), .id = \"sent_n\")) #> # A tidytable: 15 × 2 #>    sent_n word    #>     <int> <chr>   #>  1      1 The     #>  2      1 apple   #>  3      1 doesn't #>  4      1 fall    #>  5      1 far     #>  6      1 from    #>  7      1 the     #>  8      1 tree.   #>  9      2 Don't   #> 10      2 judge   #> 11      2 a       #> 12      2 book    #> 13      2 by      #> 14      2 its     #> 15      2 cover. df_sent <- df_sent |>   mutate(lp = causal_lp(word, .by = sent_n)) #> Processing using causal model 'gpt2'... #> Processing 1 batch(es) of 10 tokens. #> Text id: 1 #> `The apple doesn't fall far from the tree.` #> Processing 1 batch(es) of 9 tokens. #> Text id: 2 #> `Don't judge a book by its cover.` df_sent #> # A tidytable: 15 × 3 #>    sent_n word         lp #>     <int> <chr>     <dbl> #>  1      1 The      NA     #>  2      1 apple   -10.9   #>  3      1 doesn't  -5.50  #>  4      1 fall     -3.60  #>  5      1 far      -2.91  #>  6      1 from     -0.745 #>  7      1 the      -0.207 #>  8      1 tree.    -1.58  #>  9      2 Don't    NA     #> 10      2 judge    -6.27  #> 11      2 a        -2.33  #> 12      2 book     -1.97  #> 13      2 by       -0.409 #> 14      2 its      -0.257 #> 15      2 cover.   -1.38"},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"how-to-cite","dir":"","previous_headings":"","what":"How to cite","title":"Access to Large Language Model Predictions in R","text":"Nicenboim B (2023). pangoling: Access language model predictions R. R package version 0.0.0.9001, DOI: 10.5281/zenodo.7637526, https://github.com/bnicenboim/pangoling.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of conduct","title":"Access to Large Language Model Predictions in R","text":"Please note package released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"see-also","dir":"","previous_headings":"","what":"See also","title":"Access to Large Language Model Predictions in R","text":"Another R package act wrapper transformers text However, text general, focus Natural Language Processing Machine Learning.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the configuration of a causal model — causal_config","title":"Returns the configuration of a causal model — causal_config","text":"Returns configuration causal model","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the configuration of a causal model — causal_config","text":"","code":"causal_config(   model = getOption(\"pangoling.causal.default\"),   config_model = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the configuration of a causal model — causal_config","text":"model Name pre-trained model. config_model List arguments control model Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the configuration of a causal model — causal_config","text":"list configuration model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Returns the configuration of a causal model — causal_config","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set specified global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the configuration of a causal model — causal_config","text":"","code":"if (FALSE) { # interactive() causal_config(model = \"gpt2\") }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp","text":"Get log probability element vector words (phrases) using causal transformer model. See online article pangoling website examples.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp","text":"","code":"causal_lp(   x,   .by = rep(1, length(x)),   ignore_regex = \"\",   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp","text":"x Vector words, phrases texts. .Vector indicates text split. ignore_regex Can ignore certain characters calculates log probabilities. example ^[[:punct:]]$ ignore punctuation  stands alone token. model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp","text":"named vector log probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set specified global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp.html","id":"more-examples","dir":"Reference","previous_headings":"","what":"More examples","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp","text":"See online article pangoling website examples.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — causal_lp","text":"","code":"if (FALSE) { # interactive() causal_lp(   x = c(\"The\", \"apple\", \"doesn't\", \"fall\", \"far\", \"from\", \"the\", \"tree.\"),   model = \"gpt2\" ) }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp_mats.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a list of matrices with the log probabilities of possible word given its previous context using a causal transformer — causal_lp_mats","title":"Get a list of matrices with the log probabilities of possible word given its previous context using a causal transformer — causal_lp_mats","text":"Get list matrices log probabilities possible word given previous context using causal transformer model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp_mats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a list of matrices with the log probabilities of possible word given its previous context using a causal transformer — causal_lp_mats","text":"","code":"causal_lp_mats(   x,   .by = rep(1, length(x)),   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp_mats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a list of matrices with the log probabilities of possible word given its previous context using a causal transformer — causal_lp_mats","text":"x Vector words, phrases texts. .Vector indicates text split. model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp_mats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a list of matrices with the log probabilities of possible word given its previous context using a causal transformer — causal_lp_mats","text":"list matrices tokens columns vocabulary model rows","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp_mats.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a list of matrices with the log probabilities of possible word given its previous context using a causal transformer — causal_lp_mats","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set specified global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp_mats.html","id":"more-examples","dir":"Reference","previous_headings":"","what":"More examples","title":"Get a list of matrices with the log probabilities of possible word given its previous context using a causal transformer — causal_lp_mats","text":"See online article pangoling website examples.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_lp_mats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get a list of matrices with the log probabilities of possible word given its previous context using a causal transformer — causal_lp_mats","text":"","code":"if (FALSE) { # interactive() causal_lp_mats(   x = c(\"The\", \"apple\", \"doesn't\", \"fall\", \"far\", \"from\", \"the\", \"tree.\"),   model = \"gpt2\" ) }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — causal_next_tokens_tbl","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — causal_next_tokens_tbl","text":"Get possible next tokens log probabilities based previous context using causal transformer model Hugging Face.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — causal_next_tokens_tbl","text":"","code":"causal_next_tokens_tbl(   context,   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — causal_next_tokens_tbl","text":"context context. model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — causal_next_tokens_tbl","text":"table possible next tokens log-probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_tbl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — causal_next_tokens_tbl","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set specified global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_tbl.html","id":"more-examples","dir":"Reference","previous_headings":"","what":"More examples","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — causal_next_tokens_tbl","text":"See online article pangoling website examples.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_next_tokens_tbl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — causal_next_tokens_tbl","text":"","code":"if (FALSE) { # interactive() causal_next_tokens_tbl(   context = \"The apple doesn't fall far from the\",   model = \"gpt2\" ) }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":null,"dir":"Reference","previous_headings":"","what":"Preloads a causal language model — causal_preload","title":"Preloads a causal language model — causal_preload","text":"Preloads causal language model speed next runs.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preloads a causal language model — causal_preload","text":"","code":"causal_preload(   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preloads a causal language model — causal_preload","text":"model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preloads a causal language model — causal_preload","text":"Nothing.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preloads a causal language model — causal_preload","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set specified global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_preload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preloads a causal language model — causal_preload","text":"","code":"if (FALSE) { # interactive() causal_preload(model = \"gpt2\") }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_tokens_lp_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl","text":"Get log probability token sentence (group sentences) using causal transformer model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_tokens_lp_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl","text":"","code":"causal_tokens_lp_tbl(   texts,   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL,   .id = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_tokens_lp_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl","text":"texts Vector list texts. model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed. .id Name column sentence id.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_tokens_lp_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl","text":"table token names (token), log-probability (lp) optionally sentence id.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_tokens_lp_tbl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl","text":"causal language model (also called GPT-like, auto-regressive, decoder model) type large language model usually used text-generation can predict next word (accurately fact token) based preceding context. specified, causal model used one set specified global option pangoling.causal.default, can accessed via getOption(\"pangoling.causal.default\") (default \"gpt2\"). change default option use options(pangoling.causal.default = \"newcausalmodel\"). list possible causal models can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see Python method from_pretrained details. case errors new model run, check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_tokens_lp_tbl.html","id":"more-examples","dir":"Reference","previous_headings":"","what":"More examples","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl","text":"See online article pangoling website examples.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/causal_tokens_lp_tbl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — causal_tokens_lp_tbl","text":"","code":"if (FALSE) { # interactive() causal_tokens_lp_tbl(   texts = c(\"The apple doesn't fall far from the tree.\"),   model = \"gpt2\" ) }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the configuration of a masked model — masked_config","title":"Returns the configuration of a masked model — masked_config","text":"Returns configuration masked model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the configuration of a masked model — masked_config","text":"","code":"masked_config(   model = getOption(\"pangoling.masked.default\"),   config_model = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the configuration of a masked model — masked_config","text":"model Name pre-trained model. config_model List arguments control model Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the configuration of a masked model — masked_config","text":"list configuration model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Returns the configuration of a masked model — masked_config","text":"masked language model (also called BERT-like, encoder model) type large language model  can used predict content mask sentence. specified, masked model used one set specified global option pangoling.masked.default, can accessed via getOption(\"pangoling.masked.default\") (default \"bert-base-uncased\"). change default option use options(pangoling.masked.default = \"newmaskedmodel\"). list possible masked can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see python method from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the configuration of a masked model — masked_config","text":"","code":"if (FALSE) { # interactive() masked_config(model = \"bert-base-uncased\") }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_lp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp","title":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp","text":"Get log probability vector target words (phrase) given vector left right contexts using masked transformer.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_lp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp","text":"","code":"masked_lp(   l_contexts,   targets,   r_contexts,   ignore_regex = \"\",   model = getOption(\"pangoling.masked.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_lp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp","text":"l_contexts Left context target word. targets Target words. r_contexts Right context target word. ignore_regex Can ignore certain characters calculates log probabilities. example ^[[:punct:]]$ ignore punctuation  stands alone token. model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_lp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp","text":"named vector log probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_lp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp","text":"masked language model (also called BERT-like, encoder model) type large language model  can used predict content mask sentence. specified, masked model used one set specified global option pangoling.masked.default, can accessed via getOption(\"pangoling.masked.default\") (default \"bert-base-uncased\"). change default option use options(pangoling.masked.default = \"newmaskedmodel\"). list possible masked can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see python method from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_lp.html","id":"more-examples","dir":"Reference","previous_headings":"","what":"More examples","title":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp","text":"See online article pangoling website examples.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_lp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the log probability of a target word (or phrase) given a left and right context — masked_lp","text":"","code":"if (FALSE) { # interactive() masked_lp(   l_contexts = c(\"The\", \"The\"),   targets = c(\"apple\", \"pear\"),   r_contexts = c(     \"doesn't fall far from the tree.\",     \"doesn't fall far from the tree.\"   ),   model = \"bert-base-uncased\" ) }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":null,"dir":"Reference","previous_headings":"","what":"Preloads a masked language model — masked_preload","title":"Preloads a masked language model — masked_preload","text":"Preloads masked language model speed next runs.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preloads a masked language model — masked_preload","text":"","code":"masked_preload(   model = getOption(\"pangoling.masked.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preloads a masked language model — masked_preload","text":"model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preloads a masked language model — masked_preload","text":"Nothing.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preloads a masked language model — masked_preload","text":"masked language model (also called BERT-like, encoder model) type large language model  can used predict content mask sentence. specified, masked model used one set specified global option pangoling.masked.default, can accessed via getOption(\"pangoling.masked.default\") (default \"bert-base-uncased\"). change default option use options(pangoling.masked.default = \"newmaskedmodel\"). list possible masked can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see python method from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_preload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preloads a masked language model — masked_preload","text":"","code":"if (FALSE) { # interactive() causal_preload(model = \"bert-base-uncased\") }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl","text":"mask sentence, get possible tokens log probabilities using masked transformer.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl","text":"","code":"masked_tokens_tbl(   masked_sentences,   model = getOption(\"pangoling.masked.default\"),   add_special_tokens = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl","text":"masked_sentences Masked sentences. model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_model List arguments control model Hugging Face accessed. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl","text":"table masked sentences, tokens (token), log probability (lp), respective mask number (mask_n).","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_tbl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl","text":"masked language model (also called BERT-like, encoder model) type large language model  can used predict content mask sentence. specified, masked model used one set specified global option pangoling.masked.default, can accessed via getOption(\"pangoling.masked.default\") (default \"bert-base-uncased\"). change default option use options(pangoling.masked.default = \"newmaskedmodel\"). list possible masked can found Hugging Face website. Using  config_model config_tokenizer arguments, possible control model tokenizer Hugging Face accessed, see python method from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_tbl.html","id":"more-examples","dir":"Reference","previous_headings":"","what":"More examples","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl","text":"See online article pangoling website examples.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/masked_tokens_tbl.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the possible tokens and their log probabilities for each mask in a sentence — masked_tokens_tbl","text":"","code":"if (FALSE) { # interactive() masked_tokens_tbl(\"The [MASK] doesn't fall far from the tree.\",   model = \"bert-base-uncased\" ) }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":null,"dir":"Reference","previous_headings":"","what":"The number of tokens in a string or vector of strings — ntokens","title":"The number of tokens in a string or vector of strings — ntokens","text":"number tokens string vector strings","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The number of tokens in a string or vector of strings — ntokens","text":"","code":"ntokens(   x,   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The number of tokens in a string or vector of strings — ntokens","text":"x character input model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The number of tokens in a string or vector of strings — ntokens","text":"number tokens string vector words.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The number of tokens in a string or vector of strings — ntokens","text":"","code":"if (FALSE) { # interactive() ntokens(x = c(\"The apple doesn't fall far from the tree.\"), model = \"gpt2\") }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-package.html","id":null,"dir":"Reference","previous_headings":"","what":"pangoling: Access to Large Language Model Predictions — pangoling-package","title":"pangoling: Access to Large Language Model Predictions — pangoling-package","text":"Access word predictability using large language (transformer) models.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"pangoling: Access to Large Language Model Predictions — pangoling-package","text":"Maintainer: Bruno Nicenboim bruno.nicenboim@gmail.com (ORCID) contributors: Chris Emmerly [contributor] Giovanni Cassani [contributor]","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates perplexity — perplexity_calc","title":"Calculates perplexity — perplexity_calc","text":"Calculates perplexity vector (log-)probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates perplexity — perplexity_calc","text":"","code":"perplexity_calc(x, na.rm = FALSE, log.p = TRUE)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates perplexity — perplexity_calc","text":"x vector log-probabilities. na.rm missing values (including NaN) removed? log.p TRUE (default),  x assumed log-transformed probabilities base e, FALSE x assumed raw probabilities, alternatively log.p can base logarithmic transformations.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates perplexity — perplexity_calc","text":"perplexity.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculates perplexity — perplexity_calc","text":"x raw probabilities (default), perplexity calculated follows: $$\\left(\\prod_{n} x_n \\right)^\\frac{1}{N}$$","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity_calc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculates perplexity — perplexity_calc","text":"","code":"probs <- c(.3, .5, .6) perplexity_calc(probs, log.p = FALSE) #> [1] 2.231443 lprobs <- log(probs) perplexity_calc(lprobs, log.p = TRUE) #> [1] 2.231443"},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize an input — tokenize_lst","title":"Tokenize an input — tokenize_lst","text":"Tokenize string token ids.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize an input — tokenize_lst","text":"","code":"tokenize_lst(   x,   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize an input — tokenize_lst","text":"x Strings token ids. model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenize an input — tokenize_lst","text":"list tokens","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/tokenize_lst.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenize an input — tokenize_lst","text":"","code":"if (FALSE) { # interactive() tokenize_lst(x = c(\"The apple doesn't fall far from the tree.\"), model = \"gpt2\") }"},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the vocabulary of a model — transformer_vocab","title":"Returns the vocabulary of a model — transformer_vocab","text":"Returns vocabulary model.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the vocabulary of a model — transformer_vocab","text":"","code":"transformer_vocab(   model = getOption(\"pangoling.causal.default\"),   add_special_tokens = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the vocabulary of a model — transformer_vocab","text":"model Name pre-trained model. add_special_tokens Whether include special tokens. default AutoTokenizer method Python. config_tokenizer List arguments control tokenizer Hugging Face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the vocabulary of a model — transformer_vocab","text":"vector vocabulary model.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/transformer_vocab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the vocabulary of a model — transformer_vocab","text":"","code":"if (FALSE) { # interactive() transformer_vocab(model = \"gpt2\") }"},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009000","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9000","title":"pangoling 0.0.0.9000","text":"First release!","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009001","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9001","title":"pangoling 0.0.0.9001","text":"Tons stuff. Fully functional package now.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009001-1","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9001","title":"pangoling 0.0.0.9001","text":"minor function names avoid conflict packages","code":""}]
