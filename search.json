[{"path":"http://bruno.nicenboim.me/pangoling/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 Bruno Nicenboim Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bruno Nicenboim. Author, maintainer. Chris Emmerly. Contributor. Giovanni Cassani. Contributor.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Nicenboim B (2023). pangoling: Access language model predictions R. R package version 0.0.0.9000, https://github.com/bnicenboim/pangoling.","code":"@Manual{,   title = {pangoling: Access to language model predictions in R},   author = {Bruno Nicenboim},   year = {2023},   note = {R package version 0.0.0.9000},   url = {https://github.com/bnicenboim/pangoling}, }"},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"pangoling-","dir":"","previous_headings":"","what":"Access to language model predictions in R","title":"Access to language model predictions in R","text":"pangoling1 R package estimating log-probabilities words given context using transformer models. package provides interface utilizing pre-trained transformer models (BERT GPT-2) obtain word probabilities. log-probabilities often utilized predictors psycholinguistic studies. package can useful researchers field psycholinguistics want leverage power transformer models work. package mostly wrapper python package transformers process data convenient format. moment “causal” models GPT-2 working.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"important-limitations-and-bias","dir":"","previous_headings":"","what":"Important! Limitations and bias","title":"Access to language model predictions in R","text":"training data popular models (GPT-2) haven’t released, one inspect . ’s clear data contain lot unfiltered content internet, far neutral. See example scope openAI team’s model card GPT-2, many models, limitations bias section GPT-2 hugging face website.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Access to language model predictions in R","text":"still released version pangoling. package early stages development, ’s well tested, subject lot changes. install latest version github use:","code":"# install.packages(\"remotes\") # if needed remotes::install_github(\"bnicenboim/pangoling\")"},{"path":"http://bruno.nicenboim.me/pangoling/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Access to language model predictions in R","text":"basic example shows solve common problem: intended use package following. Given (toy) dataset like . ’s straight-forward get log-probability (-suprisal) word based GPT-2.","code":"library(pangoling) library(tidytable) #fast alternative to dplyr sentences <- c(\"The apple doesn't fall far from the tree.\",                 \"Don't judge a book by its cover.\") (df_sent <- strsplit(x = sentences, split = \" \") |>    map_dfr(.f =  ~ data.frame(word = .x), .id = \"sent_n\")) #> # A tidytable: 15 × 2 #>    sent_n word    #>     <int> <chr>   #>  1      1 The     #>  2      1 apple   #>  3      1 doesn't #>  4      1 fall    #>  5      1 far     #>  6      1 from    #>  7      1 the     #>  8      1 tree.   #>  9      2 Don't   #> 10      2 judge   #> 11      2 a       #> 12      2 book    #> 13      2 by      #> 14      2 its     #> 15      2 cover. df_sent <- df_sent |>   mutate(lp = get_causal_log_prob(word, .by = sent_n)) #> Processing 1 batch(es) of 10 tokens. #> Processing using causal model 'gpt2'... #> Text id: 1 #> `The apple doesn't fall far from the tree.` #> Processing 1 batch(es) of 9 tokens. #> Processing using causal model 'gpt2'... #> Text id: 2 #> `Don't judge a book by its cover.` df_sent #> # A tidytable: 15 × 3 #>    sent_n word         lp #>     <int> <chr>     <dbl> #>  1      1 The      NA     #>  2      1 apple   -10.9   #>  3      1 doesn't  -5.50  #>  4      1 fall     -3.60  #>  5      1 far      -2.91  #>  6      1 from     -0.745 #>  7      1 the      -0.207 #>  8      1 tree.    -1.58  #>  9      2 Don't    NA     #> 10      2 judge    -6.27  #> 11      2 a        -2.33  #> 12      2 book     -1.97  #> 13      2 by       -0.409 #> 14      2 its      -0.257 #> 15      2 cover.   -1.38"},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_fix.html","id":null,"dir":"Reference","previous_headings":"","what":"Fixation positions of the eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_fix","title":"Fixation positions of the eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_fix","text":"Fixation positions eye-tracking  data Frank, Fernandez Monsalve, Thompson et al. (2013).","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_fix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fixation positions of the eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_fix","text":"","code":"data_frank2013_et_fix"},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_fix.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fixation positions of the eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_fix","text":"data frame: subj: Subject id. sent_id: Sentence unique id. gaze_x: Horizontal pixel coordinate fixation location (0 = left edge; 39 = left margin; 1024 = right edge). letter (including space punctuation) 14 pixels wide. gaze_y: Vertical pixel coordinate fixation location (0 = top display; 768 = bottom display). fix_duration: Duration fixation ms. letter_n: Fixated letter position sentence (including spaces), even fixation left left margin right last letter. Position 0 directly left first letter. word_n: Position word sentence. word: Presented word. blink: Indicates whether blink detected directly '' '' current fixation ('' ).","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_fix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fixation positions of the eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_fix","text":"original dataset: \"Within sentence, fixations listed order occurred. fixation sentence presentation registered outside display bounds, regarded tracking error data presented sentence. letter word position NaN gaze_y outside range (300,475) considered far presented sentence.\"","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_fix.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fixation positions of the eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_fix","text":"1 S. L. Frank, . Fernandez Monsalve, R. L. Thompson, et al. ``Reading Time Data Evaluating Broad-Coverage Models English Sentence Processing''. : Behavior Research Methods 45.4 (dec. 01, 2013), pp. 1182--1190. ISSN: 1554-3528. DOI: 10.3758/s13428-012-0313-y.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_rt.html","id":null,"dir":"Reference","previous_headings":"","what":"Eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_rt","title":"Eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_rt","text":"Eye-tracking reading data Frank, Fernandez Monsalve, Thompson et al. (2013)","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_rt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_rt","text":"","code":"data_frank2013_et_rt"},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_rt.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_rt","text":"data frame 81109 rows 11 columns: subj: Subject id. sent_id: Sentence unique id. sent_n: Position sentence presentation sequence. answer_time: Time ms question presentation response, NaN question. acc_comprehension: Whether answer correct . word_n: Position word sentence. word: Presented word. RTfirstfix: First fixation time current word (0 word fixated). RTfirstpass: First-pass reading time (total fixation time current word first fixation word). RTrightbound: Right-bounded reading time (total fixation time current word first fixation word right). RTgopast: Go-past reading time (total fixation time first fixation current word first fixation word right).","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_et_rt.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Eye-tracking data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_et_rt","text":"1 S. L. Frank, . Fernandez Monsalve, R. L. Thompson, et al. ``Reading Time Data Evaluating Broad-Coverage Models English Sentence Processing''. : Behavior Research Methods 45.4 (dec. 01, 2013), pp. 1182--1190. ISSN: 1554-3528. DOI: 10.3758/s13428-012-0313-y.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_spr.html","id":null,"dir":"Reference","previous_headings":"","what":"Cleaned self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr","title":"Cleaned self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr","text":"cleaned version dataset \"data_frank2013_spr_complete\": sentences typos, native speakers, subjects accuracy 0.8 Frank, Fernandez Monsalve, Thompson et al. (2013)","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_spr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cleaned self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr","text":"","code":"data_frank2013_spr"},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_spr.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Cleaned self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr","text":"object class tidytable (inherits data.table, data.frame) 165040 rows 12 columns.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_spr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cleaned self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr","text":"1 S. L. Frank, . Fernandez Monsalve, R. L. Thompson, et al. ``Reading Time Data Evaluating Broad-Coverage Models English Sentence Processing''. : Behavior Research Methods 45.4 (dec. 01, 2013), pp. 1182--1190. ISSN: 1554-3528. DOI: 10.3758/s13428-012-0313-y.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_spr_complete.html","id":null,"dir":"Reference","previous_headings":"","what":"Self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr_complete","title":"Self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr_complete","text":"dataset containing self-paced reading data Frank, Fernandez Monsalve, Thompson et al. (2013). Notice collecting self-paced-reading data, three typos found: sentence 43  : \"Scott\" \"Sott\" sentence 269 : \"\" \"\" sentence 337 : \"Margaret\" \"Margeret\"","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_spr_complete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr_complete","text":"","code":"data_frank2013_spr_complete"},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_spr_complete.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr_complete","text":"data frame  rows  variables: subj: Subject id. sent_id: Sentence unique id. sent_n: Position sentence presentation sequence. answer_time: Time ms question presentation response, NaN question. word_n: Position word sentence. word: Presented word. RT: Time ms word presentation key press. acc_comprehension: Correctness response comprehension question. age: Subject's age years. age_en: Age subject began learning English (0 native speakers). sex: Subject's sex (f/m). hand: Subject's handedness (r/l). correct_perc: Fraction correct responses comprehension questions. typo: Whether typo word.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_spr_complete.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Self-paced reading data of Frank, Fernandez Monsalve, Thompson et al. (2013) — data_frank2013_spr_complete","text":"1 S. L. Frank, . Fernandez Monsalve, R. L. Thompson, et al. ``Reading Time Data Evaluating Broad-Coverage Models English Sentence Processing''. : Behavior Research Methods 45.4 (dec. 01, 2013), pp. 1182--1190. ISSN: 1554-3528. DOI: 10.3758/s13428-012-0313-y.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_stimuli.html","id":null,"dir":"Reference","previous_headings":"","what":"Enriched  stimuli of Frank, Fernandez Monsalve, Thompson, and Vigliocco (2013) data — data_frank2013_stimuli","title":"Enriched  stimuli of Frank, Fernandez Monsalve, Thompson, and Vigliocco (2013) data — data_frank2013_stimuli","text":"dataset containing stimuli used Frank, Fernandez Monsalve, Thompson et al. (2013). enriched estimates surprisal entropy reduction Frank (2013).","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_stimuli.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enriched  stimuli of Frank, Fernandez Monsalve, Thompson, and Vigliocco (2013) data — data_frank2013_stimuli","text":"","code":"data_frank2013_stimuli"},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_stimuli.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Enriched  stimuli of Frank, Fernandez Monsalve, Thompson, and Vigliocco (2013) data — data_frank2013_stimuli","text":"data frame  rows  variables: sent_id: sentence unique id. sentence: sentence, single string question: Comprehension question (hyphen question) answer: Correct answer comprehension question (hyphen question) pos: String part--speech tags (Penn Treebank style) word_n: Position word sentence word: Presented word surprisal_1-surprisal_10: Surprisal estimate ten points (x=1 x=10) network training. deltaH1_1-deltaH4_10: Entropy-reduction estimate ten points network training, lookahead distance n.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_stimuli.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Enriched  stimuli of Frank, Fernandez Monsalve, Thompson, and Vigliocco (2013) data — data_frank2013_stimuli","text":"https://osf.io/sjefs//","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_stimuli.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Enriched  stimuli of Frank, Fernandez Monsalve, Thompson, and Vigliocco (2013) data — data_frank2013_stimuli","text":"original dataset part--speech tags generated Tsuruoka & Tsujii's (2005) automatic tagger, corrected hand (accordance Penn Treebank part--speech  guidelines; Santorini, 1991); Punctuation marks receive tag, clitics split two tags (e.g., \"\" tagged \"VBZ RB\").","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_frank2013_stimuli.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Enriched  stimuli of Frank, Fernandez Monsalve, Thompson, and Vigliocco (2013) data — data_frank2013_stimuli","text":"1 S. L. Frank. ``Uncertainty Reduction Measure Cognitive Load Sentence Comprehension''. : Topics Cognitive Science 5.3 (2013), pp. 475--494. ISSN: 1756-8765. DOI: 10.1111/tops.12025. 2 S. L. Frank, . Fernandez Monsalve, R. L. Thompson, et al. ``Reading Time Data Evaluating Broad-Coverage Models English Sentence Processing''. : Behavior Research Methods 45.4 (dec. 01, 2013), pp. 1182--1190. ISSN: 1554-3528. DOI: 10.3758/s13428-012-0313-y.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_provo_cloze.html","id":null,"dir":"Reference","previous_headings":"","what":"Cloze probabilities from the Provo Corpus (Luke and Christianson, 2018). — data_provo_cloze","title":"Cloze probabilities from the Provo Corpus (Luke and Christianson, 2018). — data_provo_cloze","text":"dataset adapted  Provo Corpus (Luke Christianson, 2018) word probability stimuli eye-tracking experiment.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_provo_cloze.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cloze probabilities from the Provo Corpus (Luke and Christianson, 2018). — data_provo_cloze","text":"","code":"data_provo_cloze"},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_provo_cloze.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Cloze probabilities from the Provo Corpus (Luke and Christianson, 2018). — data_provo_cloze","text":"data frame 2730 rows 2730 columns: word_unique_id:  unique ID number word (token) data set. text_id:  text number (paragraph 1–55). word_n:  ordinal position word text. word:   target word appeared screen eye-tracking experiment. word_cleaned: target word, punctuation, capitalization contractions removed. word_length: Word length. total_response_count:  total number responses provided cloze task word. cp:  often given response provided, proportion responses. Can used measure predictability. cp = response_count/total_response_count modal_response: modal response. pos_claws: part speech tag target word. (See http://ucrel.lancs.ac.uk/claws/ information meaning specific tags.) word_content_or_function:  Whether word content word function word, based pos_claws. word_pos:  general grouping parts speech, based POS_CLAWS, includes following categories: adjective, adverb, article, conjunction, determiner, existential, infinitive marker, negative, noun, number, preposition, pronoun, verb. pos_cp:  proportion responses POS target, using word_pos (e.g., Target response nouns). lsa_context_score:  measure semantic association target word entire preceding passage context, obtained using latent semantic analysis (Landauer & Dumais, 1997; http://lsa.colorado.edu/). example, LSA score word \"rumblings\" obtained comparing “rumblings” preceding context \"now.\" score measure contextual fit given target word. lsa_response_match_score:  mean LSA match score target provided responses. example, pairwise LSA used compare target “carts” responses provided cloze procedure (e.g., “horses,” “slower,” “”), LSA scores responses averaged. measure estimate semantic predictability given target word (.e., participants good sense general meaning upcoming word, even predict exactly word ). text:  entire text target word taken.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_provo_cloze.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Cloze probabilities from the Provo Corpus (Luke and Christianson, 2018). — data_provo_cloze","text":"https://osf.io/sjefs//","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/data_provo_cloze.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cloze probabilities from the Provo Corpus (Luke and Christianson, 2018). — data_provo_cloze","text":"1 S. G. Luke K. Christianson. ``Provo Corpus: large eye-tracking corpus predictability norms''. : Behavior research methods 50 (2018), pp. 826--833.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — get_causal_log_prob","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — get_causal_log_prob","text":"Get log probability element vector words (phrases) using causal transformer model. See vignette(\"transformer-gpt2\", package = \"pangoling\") examples.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — get_causal_log_prob","text":"","code":"get_causal_log_prob(   x,   .by = rep(1, length(x)),   ignore_regex = \"\",   model = \"gpt2\",   add_bos_token = NULL,   stride = 1,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — get_causal_log_prob","text":"x Vector words, phrases texts. .Vector indicates text split. ignore_regex Can ignore certain characters calculates log probabilities. example ^[[:punct:]]$ ignore punctuation  stands alone token. model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.) add_bos_token Whether include beginning text special tokens. default  acts AutoTokenizer. config_model List arguments control model hugging face accessed. config_tokenizer List arguments control tokenizer hugging face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — get_causal_log_prob","text":"named vector log probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the log probability of each element of a vector of words (or phrases) using a causal transformer — get_causal_log_prob","text":"causal models, see (https://huggingface.co/course/chapter7/6).  Using  config_model config_tokenizer arguments, possible control model tokenizer hugging face accessed, see from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob_mat.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a matrix with the log probabilities of possible word given its previous context using a causal transformer — get_causal_log_prob_mat","title":"Get a matrix with the log probabilities of possible word given its previous context using a causal transformer — get_causal_log_prob_mat","text":"Get matrix log probabilities possible word given previous context using causal transformer model Hugging Face.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob_mat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a matrix with the log probabilities of possible word given its previous context using a causal transformer — get_causal_log_prob_mat","text":"","code":"get_causal_log_prob_mat(   x,   .by = rep(1, length(x)),   model = \"gpt2\",   add_bos_token = NULL,   stride = 1,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob_mat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a matrix with the log probabilities of possible word given its previous context using a causal transformer — get_causal_log_prob_mat","text":"x Vector words, phrases texts. .Vector indicates text split. model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.) add_bos_token Whether include beginning text special tokens. default  acts AutoTokenizer. config_model List arguments control model hugging face accessed. config_tokenizer List arguments control tokenizer hugging face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob_mat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a matrix with the log probabilities of possible word given its previous context using a causal transformer — get_causal_log_prob_mat","text":"matrix.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_log_prob_mat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a matrix with the log probabilities of possible word given its previous context using a causal transformer — get_causal_log_prob_mat","text":"causal models, see (https://huggingface.co/course/chapter7/6). Using  config_model config_tokenizer arguments, possible control model tokenizer hugging face accessed, see from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_model_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the configuration of a causal model — get_causal_model_config","title":"Returns the configuration of a causal model — get_causal_model_config","text":"Returns configuration causal model","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_model_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the configuration of a causal model — get_causal_model_config","text":"","code":"get_causal_model_config(model = \"gpt2\", config = NULL)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_model_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the configuration of a causal model — get_causal_model_config","text":"model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.)","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_model_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the configuration of a causal model — get_causal_model_config","text":"list configuration model","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_next_tokens_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — get_causal_next_tokens_tbl","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — get_causal_next_tokens_tbl","text":"Get possible next tokens log probabilities previous context using causal transformer model Hugging Face.  Get log probability word phrase vector given previous context using transformer model huggingface.co/. See vignette(\"transformer-gpt2\", package = \"pangoling\") examples.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_next_tokens_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — get_causal_next_tokens_tbl","text":"","code":"get_causal_next_tokens_tbl(   context,   model = \"gpt2\",   add_bos_token = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_next_tokens_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — get_causal_next_tokens_tbl","text":"context Context. model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.) add_bos_token Whether include beginning text special tokens. default  acts AutoTokenizer. config_model List arguments control model hugging face accessed. config_tokenizer List arguments control tokenizer hugging face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_next_tokens_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — get_causal_next_tokens_tbl","text":"table possible next tokens log-probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_next_tokens_tbl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the possible next tokens and their log probabilities its previous context using a causal transformer — get_causal_next_tokens_tbl","text":"causal models, see (https://huggingface.co/course/chapter7/6). Using  config_model config_tokenizer arguments, possible control model tokenizer hugging face accessed, see from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_tokens_log_prob_tbl.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — get_causal_tokens_log_prob_tbl","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — get_causal_tokens_log_prob_tbl","text":"Get log probability token sentence (group sentences) using causal transformer model. See vignette(\"transformer-gpt2\", package = \"pangoling\") examples.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_tokens_log_prob_tbl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — get_causal_tokens_log_prob_tbl","text":"","code":"get_causal_tokens_log_prob_tbl(   texts,   model = \"gpt2\",   add_bos_token = NULL,   stride = 1,   config_model = NULL,   config_tokenizer = NULL,   .id = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_tokens_log_prob_tbl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — get_causal_tokens_log_prob_tbl","text":"texts Vector list texts. model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.) add_bos_token Whether include beginning text special tokens. default  acts AutoTokenizer. config_model List arguments control model hugging face accessed. config_tokenizer List arguments control tokenizer hugging face accessed. .id Column sentence id.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_tokens_log_prob_tbl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — get_causal_tokens_log_prob_tbl","text":"table token names, log-probability optionally sentence id.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_causal_tokens_log_prob_tbl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the log probability of each token in a sentence (or group of sentences) using a causal transformer — get_causal_tokens_log_prob_tbl","text":"causal models, see (https://huggingface.co/course/chapter7/6).  Using  config_model config_tokenizer arguments, possible control model tokenizer hugging face accessed, see from_pretrained details. case errors check status https://status.huggingface.co/","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenize the input — get_tokens","title":"Tokenize the input — get_tokens","text":"Tokenize input","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenize the input — get_tokens","text":"","code":"get_tokens(x, model = \"gpt2\", add_bos_token = NULL, config = NULL)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenize the input — get_tokens","text":"x Strings token ids. model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.) add_bos_token Whether include beginning text special tokens. default  acts AutoTokenizer. config List arguments control tokenizer hugging face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_tr_vocab.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the vocabulary of a model — get_tr_vocab","title":"Returns the vocabulary of a model — get_tr_vocab","text":"Returns vocabulary model","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_tr_vocab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the vocabulary of a model — get_tr_vocab","text":"","code":"get_tr_vocab(model = \"gpt2\", add_bos_token = NULL, config = NULL)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_tr_vocab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the vocabulary of a model — get_tr_vocab","text":"model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.) add_bos_token Whether include beginning text special tokens. default  acts AutoTokenizer.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/get_tr_vocab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the vocabulary of a model — get_tr_vocab","text":"vector vocabulary model","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":null,"dir":"Reference","previous_headings":"","what":"The number of tokens in a string or vector of strings — ntokens","title":"The number of tokens in a string or vector of strings — ntokens","text":"number tokens string vector strings","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The number of tokens in a string or vector of strings — ntokens","text":"","code":"ntokens(x, model = \"gpt2\", add_bos_token = NULL, config = NULL)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The number of tokens in a string or vector of strings — ntokens","text":"x character input model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.) add_bos_token Whether include beginning text special tokens. default  acts AutoTokenizer. config List arguments control tokenizer hugging face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/ntokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The number of tokens in a string or vector of strings — ntokens","text":"number tokens string vector words.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-package.html","id":null,"dir":"Reference","previous_headings":"","what":"pangoling: Access to language model predictions in R — pangoling-package","title":"pangoling: Access to language model predictions in R — pangoling-package","text":"Access large language model word predictability R.","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/pangoling-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"pangoling: Access to language model predictions in R — pangoling-package","text":"Maintainer: Bruno Nicenboim bruno.nicenboim@gmail.com (ORCID) contributors: Chris Emmerly [contributor] Giovanni Cassani [contributor]","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates perplexity — perplexity","title":"Calculates perplexity — perplexity","text":"Calculates perplexity vector (log-)probabilities.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates perplexity — perplexity","text":"","code":"perplexity(x, na.rm = FALSE, log.p = TRUE)"},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates perplexity — perplexity","text":"x vector log-probabilities. na.rm missing values (including NaN) removed? log.p TRUE (default),  x assumed log-transformed probabilities base e, FALSE x assumed raw probabilities, alternatively log.p can base logarithmic transformations.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates perplexity — perplexity","text":"perplexity","code":""},{"path":[]},{"path":"http://bruno.nicenboim.me/pangoling/reference/perplexity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculates perplexity — perplexity","text":"","code":"probs <- c(.3,.5,.6) perplexity(probs, log.p = FALSE) #> [1] 2.231443 lprobs <- log(probs) perplexity(lprobs, log.p = TRUE) #> [1] 2.231443"},{"path":"http://bruno.nicenboim.me/pangoling/reference/preload_causal.html","id":null,"dir":"Reference","previous_headings":"","what":"Preloads a causal language model — preload_causal","title":"Preloads a causal language model — preload_causal","text":"Preloads causal language model speed next runs.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/preload_causal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preloads a causal language model — preload_causal","text":"","code":"preload_causal(   model = \"gpt2\",   add_bos_token = NULL,   config_model = NULL,   config_tokenizer = NULL )"},{"path":"http://bruno.nicenboim.me/pangoling/reference/preload_causal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preloads a causal language model — preload_causal","text":"model Name pretrained model stored huggingface.co. (Maybe path  model (.pt .bin file) stored locally work.) add_bos_token Whether include beginning text special tokens. default  acts AutoTokenizer. config_model List arguments control model hugging face accessed. config_tokenizer List arguments control tokenizer hugging face accessed.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/reference/preload_causal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preloads a causal language model — preload_causal","text":"Nothing.","code":""},{"path":"http://bruno.nicenboim.me/pangoling/news/index.html","id":"pangoling-0009000","dir":"Changelog","previous_headings":"","what":"pangoling 0.0.0.9000","title":"pangoling 0.0.0.9000","text":"First release!","code":""}]
